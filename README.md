# Ï€â‚€ ëª¨ë¸ ë°ì´í„° íë¦„ Step-by-Step ì™„ì „ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” Ï€â‚€ ëª¨ë¸ì—ì„œ **ì…ë ¥ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ì–´ ìµœì¢… ì¶œë ¥ì´ ë˜ëŠ”ì§€**ë¥¼ í•œ ë‹¨ê³„ì”© ì¶”ì í•©ë‹ˆë‹¤.

> **ğŸ“Œ í•™ìŠµ vs ì¶”ë¡  êµ¬ë¶„**
> ì´ ë¬¸ì„œëŠ” **í•™ìŠµ(Training)** ê³¼ì •ì„ ê¸°ë³¸ìœ¼ë¡œ ì„¤ëª…í•˜ë©°, ê° Stepì—ì„œ ì¶”ë¡ (Inference)ê³¼ ì°¨ì´ê°€ ìˆëŠ” ê²½ìš° `ğŸ”„ í•™ìŠµ vs ì¶”ë¡ ` ë°•ìŠ¤ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
> - ğŸ‹ï¸ **í•™ìŠµ**: Ground truth actions + noise â†’ Flow Matching loss ê³„ì‚°
> - ğŸ¯ **ì¶”ë¡ **: Pure noiseì—ì„œ ì‹œì‘ â†’ 10íšŒ Euler integrationìœ¼ë¡œ action ìƒì„±

Step 0: ì›ë³¸ ì…ë ¥ ë°ì´í„° (Images, State, Text, Actions)
Step 1: Observation ê°ì²´ ìƒì„± (uint8 â†’ float32 ì •ê·œí™”)
Step 2: Image Embedding (SigLIP) - 3Ã—256 = 768 tokens
Step 3: Text Embedding (Gemma Embedder) - 16 tokens
Step 4: Prefix Concatenation - 784 tokens (Image + Text)
Step 5: Action Embedding (Suffix) - 32 tokens + Flow Matching
Step 6: Attention Mask ìƒì„± - [4, 816, 816]
Step 7: Transformer Layer 0 ìƒì„¸ ë¶„ì„
7-1: Pre-Attention RMSNorm (AdaRMS)
7-2: QKV Projection (Multi-Expert)
7-3: RoPE (Rotary Position Embedding)
7-4: Grouped Query Attention
7-5: Output Projection (Expertë³„)
7-6: Gated Residual
7-7: FeedForward Network
Step 8: Transformer Layers 1-17 (18 layers total)
Step 9: Final Layer Normalization
Step 10-11: Velocity Prediction + Flow Matching Loss


**ì˜ˆì‹œ ë°ì´í„°**:
- Batch Size: B = 4
- Images: 3ê°œ (base_0, left_wrist_0, right_wrist_0)
- Text: 16 tokens
- Actions: 32 timesteps, 7 DoF
- Model: Ï€â‚€.â‚… (with AdaRMS)

---

## ğŸ“ Step 0: ì›ë³¸ ì…ë ¥ ë°ì´í„°

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 0: Raw Input (Python Dictionary)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

raw_input = {
    # â”€â”€â”€ Images â”€â”€â”€
    "image": {
        "base_0_rgb": np.array([4, 224, 224, 3], dtype=uint8),        # [0, 255]
        "left_wrist_0_rgb": np.array([4, 224, 224, 3], dtype=uint8),
        "right_wrist_0_rgb": np.array([4, 224, 224, 3], dtype=uint8),
    },
    "image_mask": {
        "base_0_rgb": np.array([True, True, True, True]),
        "left_wrist_0_rgb": np.array([True, True, True, True]),
        "right_wrist_0_rgb": np.array([True, True, True, True]),
    },

    # â”€â”€â”€ Robot State â”€â”€â”€
    "state": np.array([4, 7], dtype=float32),  # [x, y, z, qx, qy, qz, gripper]

    # â”€â”€â”€ Language Command â”€â”€â”€
    "tokenized_prompt": np.array([
        [15234, 67, 123, 9876, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # "pick up fork" + padding
        [8921, 456, 789, 234, 567, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # "grasp red cup" + padding
        [...],
        [...],
    ], dtype=int32),  # [4, 16]
    "tokenized_prompt_mask": np.array([
        [True, True, True, True, False, False, ...],  # ì²« 4ê°œë§Œ valid
        [True, True, True, True, True, False, ...],   # ì²« 5ê°œë§Œ valid
        [...],
        [...],
    ], dtype=bool),  # [4, 16]

    # â”€â”€â”€ Actions (Training only) â”€â”€â”€
    "actions": np.array([4, 32, 7], dtype=float32),  # Ground truth actions
}
```

> **ğŸ”„ í•™ìŠµ vs ì¶”ë¡ **
> | | í•™ìŠµ (Training) | ì¶”ë¡  (Inference) |
> |---|---|---|
> | **Images** | ë™ì¼ | ë™ì¼ |
> | **State** | ë™ì¼ | ë™ì¼ |
> | **Text** | ë™ì¼ | ë™ì¼ |
> | **Actions** | âœ… Ground truth í•„ìš” | âŒ ì—†ìŒ (noiseì—ì„œ ìƒì„±) |

---

## ğŸ“ Step 1: Observation ê°ì²´ ìƒì„±

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/model.py:110-125`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 1: Dictionary â†’ Observation Object
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

observation = Observation.from_dict(raw_input)

# â”€â”€â”€ ë‚´ë¶€ ì²˜ë¦¬ â”€â”€â”€
# 1. uint8 ì´ë¯¸ì§€ë¥¼ float32 [-1, 1]ë¡œ ì •ê·œí™”
for key in raw_input["image"]:
    image = raw_input["image"][key]  # [4, 224, 224, 3] uint8 [0, 255]
    image = image.astype(float32) / 255.0 * 2.0 - 1.0  # float32 [-1, 1]

# 2. êµ¬ì¡°í™”ëœ Observation ê°ì²´ ìƒì„±
observation = Observation(
    images={
        "base_0_rgb": [4, 224, 224, 3],        # float32, [-1, 1]
        "left_wrist_0_rgb": [4, 224, 224, 3],
        "right_wrist_0_rgb": [4, 224, 224, 3],
    },
    image_masks={
        "base_0_rgb": [4],        # bool
        "left_wrist_0_rgb": [4],
        "right_wrist_0_rgb": [4],
    },
    state=[4, 7],                              # float32
    tokenized_prompt=[4, 16],                  # int32
    tokenized_prompt_mask=[4, 16],             # bool
)

# âœ… Output Shape:
# - Images: 3ê°œ Ã— [4, 224, 224, 3] float32 [-1, 1]
# - State: [4, 7] float32
# - Text: [4, 16] int32
```

---

## ğŸ“ Step 2: Image Embedding (SigLIP)

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:113-125` + `src/openpi/models/siglip.py`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 2: Images â†’ Image Tokens (SigLIP Vision Encoder)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ê° ì´ë¯¸ì§€ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬
image_tokens_list = []
for image_name in observation.images:
    image = observation.images[image_name]  # [4, 224, 224, 3]

    # â”€â”€â”€ SigLIP Forward Pass â”€â”€â”€
    image_tokens, _ = self.PaliGemma.img(image, train=False)

    # â”€â”€â”€ SigLIP ë‚´ë¶€ ì²˜ë¦¬ â”€â”€â”€
    # 2-1. Patch Embedding
    # Image [4, 224, 224, 3] â†’ 14Ã—14 patches
    patches = einops.rearrange(
        image,
        'b (h p1) (w p2) c -> b (h w) (p1 p2 c)',
        p1=14, p2=14
    )
    # patches: [4, 256, 588]  (256 = 16Ã—16, 588 = 14Ã—14Ã—3)

    patch_emb = nn.Dense(1152)(patches)  # [4, 256, 1152]

    # 2-2. Positional Embedding (Sinusoidal 2D)
    h, w = 16, 16  # 224/14 = 16
    y, x = jnp.mgrid[:h, :w]  # [16, 16]
    omega = jnp.arange(1152 // 4) / (1152 // 4 - 1)
    omega = 1.0 / (10000 ** omega)

    y_emb = jnp.einsum("m,d->md", y.flatten(), omega)  # [256, 288]
    x_emb = jnp.einsum("m,d->md", x.flatten(), omega)  # [256, 288]
    pos_emb = jnp.concatenate([
        jnp.sin(x_emb), jnp.cos(x_emb),
        jnp.sin(y_emb), jnp.cos(y_emb)
    ], axis=1)  # [256, 1152]

    x = patch_emb + pos_emb[None, :, :]  # [4, 256, 1152]

    # 2-3. Transformer Encoder (12 layers)
    for layer in range(12):
        # Pre-Norm
        x_norm = nn.LayerNorm()(x)

        # Multi-Head Self-Attention
        q = k = v = x_norm  # [4, 256, 1152]
        attn_out = nn.MultiHeadDotProductAttention(
            num_heads=16,  # 1152 / 16 = 72 per head
        )(q, k)  # [4, 256, 1152]

        x = x + attn_out  # Residual

        # Pre-Norm
        x_norm = nn.LayerNorm()(x)

        # MLP
        mlp_out = nn.Dense(4608)(x_norm)  # [4, 256, 4608]
        mlp_out = nn.gelu(mlp_out)
        mlp_out = nn.Dense(1152)(mlp_out)  # [4, 256, 1152]

        x = x + mlp_out  # Residual

    # 2-4. Final Projection to Gemma dimension
    image_tokens = nn.Dense(2048)(x)  # [4, 256, 2048]

    image_tokens_list.append(image_tokens)

# âœ… Output:
# image_tokens_list = [
#     [4, 256, 2048],  # base_0_rgb
#     [4, 256, 2048],  # left_wrist_0_rgb
#     [4, 256, 2048],  # right_wrist_0_rgb
# ]
# Total: 3 Ã— 256 = 768 image tokens
```

---

## ğŸ“ Step 3: Text Embedding (Gemma Embedder)

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:148-154` + `pi0.py:128-133`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 3: Token IDs â†’ Text Embeddings
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì…ë ¥: observation.tokenized_prompt
# [4, 16] int32
tokenized_prompt = observation.tokenized_prompt

# â”€â”€â”€ Embedder.encode() â”€â”€â”€
tokenized_inputs = self.PaliGemma.llm(tokenized_prompt, method="embed")

# â”€â”€â”€ Embedder ë‚´ë¶€ â”€â”€â”€
class Embedder:
    def encode(self, x):
        # 1. Embedding table lookup
        # input_embedding_table: [257152, 2048]
        x = self.input_embedding_table[(x,)]  # [4, 16, 2048]

        # 2. Scale by âˆšembed_dim (Attention Is All You Need ë…¼ë¬¸)
        x *= jnp.sqrt(2048)  # â‰ˆ 45.25
        # ì´ìœ : Embedding ê°’ì˜ scaleì„ ì¡°ì •í•˜ì—¬ position encodingê³¼ ê· í˜•

        return x  # [4, 16, 2048]

# âœ… Output:
# text_tokens: [4, 16, 2048]
#
# ì˜ˆì‹œ ë³€í™˜:
# Token ID 15234 â†’ embedding_table[15234] â†’ [2048 dims] Ã— 45.25
# Token ID 67    â†’ embedding_table[67]    â†’ [2048 dims] Ã— 45.25
# Token ID 0     â†’ embedding_table[0]     â†’ [2048 dims] Ã— 45.25 (padding)
```

---

## ğŸ“ Step 4: Prefix Token Concatenation

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:106-137`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 4: Image + Text â†’ Prefix Sequence
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def embed_prefix(self, obs):
    tokens = []
    input_mask = []
    ar_mask = []

    # â”€â”€â”€ 4-1: Image tokens ì¶”ê°€ â”€â”€â”€
    for name in obs.images:
        image_tokens = image_tokens_list.pop(0)  # [4, 256, 2048]
        tokens.append(image_tokens)

        # Mask: ëª¨ë“  image tokenì€ valid
        mask = einops.repeat(
            obs.image_masks[name],  # [4]
            "b -> b s",
            s=256
        )  # [4, 256]
        input_mask.append(mask)

        # AR Mask: imageëŠ” bidirectional attention
        ar_mask += [False] * 256

    # â”€â”€â”€ 4-2: Text tokens ì¶”ê°€ â”€â”€â”€
    if obs.tokenized_prompt is not None:
        text_tokens = tokenized_inputs  # [4, 16, 2048]
        tokens.append(text_tokens)
        input_mask.append(obs.tokenized_prompt_mask)  # [4, 16]

        # AR Mask: textë„ bidirectional attention
        ar_mask += [False] * 16

    # â”€â”€â”€ 4-3: Concatenation â”€â”€â”€
    prefix_tokens = jnp.concatenate(tokens, axis=1)
    # [4, 768, 2048] + [4, 16, 2048] = [4, 784, 2048]

    prefix_mask = jnp.concatenate(input_mask, axis=1)  # [4, 784]
    prefix_ar_mask = jnp.array(ar_mask)  # [784]

    return prefix_tokens, prefix_mask, prefix_ar_mask

# âœ… Output:
# - prefix_tokens: [4, 784, 2048]
#   â”œâ”€ Image 0: tokens[0:256]
#   â”œâ”€ Image 1: tokens[256:512]
#   â”œâ”€ Image 2: tokens[512:768]
#   â””â”€ Text:    tokens[768:784]
# - prefix_mask: [4, 784] (all True)
# - prefix_ar_mask: [784] (all False = bidirectional)
```

---

## ğŸ“ Step 5: Action Embedding (Suffix)

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:139-186`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 5: Actions â†’ Action Tokens (Suffix)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Training input: ground truth actions
actions = raw_input["actions"]  # [4, 32, 7]

# â”€â”€â”€ 5-1: Flow Matching Preparation â”€â”€â”€
# Noise ìƒì„±
rng, noise_rng, time_rng = jax.random.split(rng, 3)
noise = jax.random.normal(noise_rng, actions.shape)  # [4, 32, 7]

# Timestep ìƒ˜í”Œë§ (Beta distribution)
time = jax.random.beta(time_rng, 1.5, 1.0, batch_shape=[4])
# time: [4]  ì˜ˆ: [0.234, 0.891, 0.456, 0.123]

# Flow interpolation: x_t = tÂ·noise + (1-t)Â·actions
# - t=1: pure noise
# - t=0: real actions
# - 0<t<1: interpolated
time_expanded = time[:, None, None]  # [4, 1, 1]
x_t = time_expanded * noise + (1 - time_expanded) * actions
# x_t: [4, 32, 7]

# â”€â”€â”€ 5-2: Action Token Projection â”€â”€â”€
action_tokens = self.action_in_proj(x_t)
# Linear(in=7, out=2048)
# action_tokens: [4, 32, 2048]

# â”€â”€â”€ 5-3: Timestep Embedding (Sinusoidal) â”€â”€â”€
def posemb_sincos(pos, embedding_dim, min_period, max_period):
    # pos: [4]
    # embedding_dim: 2048

    fraction = jnp.linspace(0.0, 1.0, embedding_dim // 2)  # [1024]
    period = min_period * (max_period / min_period) ** fraction
    # period: [0.004, ..., 4.0]  (1024 values)

    sinusoid_input = jnp.einsum(
        "i,j->ij",
        pos,  # [4]
        1.0 / period * 2 * jnp.pi  # [1024]
    )  # [4, 1024]

    emb = jnp.concatenate([
        jnp.sin(sinusoid_input),  # [4, 1024]
        jnp.cos(sinusoid_input),  # [4, 1024]
    ], axis=-1)  # [4, 2048]

    return emb

time_emb = posemb_sincos(time, 2048, min_period=4e-3, max_period=4.0)
# time_emb: [4, 2048]

# â”€â”€â”€ 5-4: Ï€â‚€.â‚… AdaRMS Conditioning â”€â”€â”€
# Time MLP for AdaRMS
time_emb = self.time_mlp_in(time_emb)   # Linear(2048 â†’ 2048)
time_emb = nnx.swish(time_emb)          # Swish activation
time_emb = self.time_mlp_out(time_emb)  # Linear(2048 â†’ 2048)
time_emb = nnx.swish(time_emb)
# time_emb: [4, 2048]

action_expert_tokens = action_tokens  # [4, 32, 2048]
adarms_cond = time_emb  # [4, 2048]  â† AdaRMSì—ì„œ ì‚¬ìš©

# â”€â”€â”€ 5-5: Suffix êµ¬ì„± â”€â”€â”€
suffix_tokens = action_expert_tokens  # [4, 32, 2048]
suffix_mask = jnp.ones([4, 32], dtype=bool)  # ëª¨ë‘ valid

# AR Mask: First token is causal boundary, rest can attend to each other
suffix_ar_mask = jnp.array([True] + [False] * 31)
# [True, False, False, ..., False]
#  ^^^^  ^^^^^^^^^^^^^^^^^^^^^^^^
# Causal  Bidirectional within action block

# âœ… Output:
# - suffix_tokens: [4, 32, 2048]
# - suffix_mask: [4, 32] (all True)
# - suffix_ar_mask: [32] ([True, False, False, ...])
# - adarms_cond: [4, 2048] (for AdaRMS conditioning)
```

> **ğŸ”„ í•™ìŠµ vs ì¶”ë¡ **
> | | í•™ìŠµ (Training) | ì¶”ë¡  (Inference) |
> |---|---|---|
> | **ì…ë ¥ actions** | Ground truth actions | ì—†ìŒ (noiseì—ì„œ ì‹œì‘) |
> | **Noise** | `noise ~ N(0, I)` | `noise ~ N(0, I)` (= ì´ˆê¸° x_t) |
> | **Timestep t** | `t ~ Beta(1.5, 1.0)` ëœë¤ ìƒ˜í”Œë§ | `t = 1.0, 0.9, ..., 0.1` ê³ ì • ìŠ¤ì¼€ì¤„ |
> | **x_t ê³„ì‚°** | `x_t = tÂ·noise + (1-t)Â·actions` (interpolation) | ë°˜ë³µë§ˆë‹¤ Euler stepìœ¼ë¡œ ì—…ë°ì´íŠ¸ |
> | **íšŸìˆ˜** | **1íšŒ** (í•œ ë²ˆì˜ forward pass) | **10íšŒ** ë°˜ë³µ (ë§¤ë²ˆ suffix ì¬ìƒì„±) |
>
> í•™ìŠµì—ì„œëŠ” ëœë¤ të¡œ interpolated sampleì„ ë§Œë“¤ì§€ë§Œ, ì¶”ë¡ ì—ì„œëŠ” t=1.0(pure noise)ì—ì„œ ì‹œì‘í•˜ì—¬ ë§¤ stepë§ˆë‹¤ velocityë¥¼ ì˜ˆì¸¡í•˜ê³  `x_{t+dt} = x_t + dtÂ·v_t`ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

---

## ğŸ“ Step 6: Attention Mask ìƒì„±

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:19-44` + `202-208`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 6: Create Attention Mask
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€ 6-1: Concatenate masks â”€â”€â”€
input_mask = jnp.concatenate([prefix_mask, suffix_mask], axis=1)
# [4, 784] + [4, 32] = [4, 816]

ar_mask = jnp.concatenate([prefix_ar_mask, suffix_ar_mask], axis=0)
# [784] + [32] = [816]
# ar_mask = [False, False, ..., False, True, False, False, ..., False]
#            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  ^^^^  ^^^^^^^^^^^^^^^^^^^^^^^^
#            Prefix (bidirectional)        |     Suffix (bidirectional)
#                                      Causal boundary

# â”€â”€â”€ 6-2: Generate Attention Mask â”€â”€â”€
def make_attn_mask(input_mask, mask_ar):
    # input_mask: [4, 816]
    # mask_ar: [816]

    mask_ar = jnp.broadcast_to(mask_ar, input_mask.shape)  # [4, 816]

    # Cumulative sum: marks causal boundaries
    cumsum = jnp.cumsum(mask_ar, axis=1)
    # Example for one sample:
    # cumsum = [0, 0, ..., 0, 1, 1, 1, ..., 1]
    #          ^^^^^^^^^^^   ^^^^^^^^^^^^^^^^
    #          Prefix (0)    Suffix (1)

    # Create causal mask
    attn_mask = cumsum[:, None, :] <= cumsum[:, :, None]
    # [4, 1, 816] <= [4, 816, 1] â†’ [4, 816, 816]

    # Apply padding mask
    valid_mask = input_mask[:, None, :] * input_mask[:, :, None]
    # [4, 816, 816]

    return jnp.logical_and(attn_mask, valid_mask)

attn_mask = make_attn_mask(input_mask, ar_mask)
# attn_mask: [4, 816, 816]

# â”€â”€â”€ 6-3: Attention Mask ì‹œê°í™” â”€â”€â”€
"""
Attention pattern for one sample [816, 816]:

              Prefix(784)              Suffix(32)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Img0  Img1  Img2  Text â”‚ Act1 ... Act32â”‚
         â”‚ 0-255 256-  512-  768- â”‚ 784  ... 815  â”‚
         â”‚       511   767   783  â”‚               â”‚
    â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Prefix   â”‚  âœ“     âœ“     âœ“     âœ“  â”‚  âœ“  ...  âœ“   â”‚ â† PrefixëŠ” ëª¨ë“  ê²ƒì„
0-783    â”‚  âœ“     âœ“     âœ“     âœ“  â”‚  âœ“  ...  âœ“   â”‚   ë³¼ ìˆ˜ ìˆìŒ
         â”‚  ...                   â”‚  ...          â”‚   (Bidirectional)
    â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Suffix   â”‚  âœ“     âœ“     âœ“     âœ“  â”‚  âœ“   âœ—   âœ—   â”‚ â† SuffixëŠ” prefixëŠ”
784      â”‚  âœ“     âœ“     âœ“     âœ“  â”‚  âœ“   âœ“   âœ—   â”‚   ë³¼ ìˆ˜ ìˆì§€ë§Œ,
785      â”‚  âœ“     âœ“     âœ“     âœ“  â”‚  âœ“   âœ“   âœ“   â”‚   suffix ë‚´ì—ì„œëŠ”
...      â”‚  ...                   â”‚  ...          â”‚   causal
815      â”‚  âœ“     âœ“     âœ“     âœ“  â”‚  âœ“   âœ“   âœ“   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     ^^^^^^^^^^
                                     Causal mask
"""

# â”€â”€â”€ 6-4: Position Encoding â”€â”€â”€
positions = jnp.cumsum(input_mask, axis=1) - 1
# [4, 816]
# Example: [[0, 1, 2, ..., 783, 784, 785, ..., 815], ...]

# âœ… Output:
# - attn_mask: [4, 816, 816] bool
# - positions: [4, 816] int32
```

---

## ğŸ“ Step 7: Multi-Expert Transformer Layer 0

ì´ì œ 18ê°œì˜ Transformer layer ì¤‘ **ì²« ë²ˆì§¸ layer**ë¥¼ ìì„¸íˆ ë´…ë‹ˆë‹¤.

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:284-333`

### Step 7-1: Pre-Attention RMSNorm

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-1: Pre-Attention RMSNorm (with AdaRMS)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì…ë ¥:
xs = [prefix_tokens, suffix_tokens]
# xs[0]: [4, 784, 2048]  (Expert 0 - PaliGemma)
# xs[1]: [4, 32, 2048]   (Expert 1 - Action)

adarms_cond = [None, adarms_cond]
# adarms_cond[0]: None (Expert 0ëŠ” conditioning ì•ˆ í•¨)
# adarms_cond[1]: [4, 2048] (Expert 1ì€ timestep conditioning)

# â”€â”€â”€ RMSNorm ì ìš© â”€â”€â”€
pre_attn = []
gates = []

for i, x in enumerate(xs):
    if x is not None:
        # 1. Root Mean Square ê³„ì‚°
        var = jnp.mean(jnp.square(x.astype(float32)), axis=-1, keepdims=True)
        # xs[0]: [4, 784, 2048] â†’ var: [4, 784, 1]
        # xs[1]: [4, 32, 2048]  â†’ var: [4, 32, 1]

        # 2. Normalization
        normed_inputs = x * jnp.reciprocal(jnp.sqrt(var + 1e-6))
        # xs[0]: [4, 784, 2048]
        # xs[1]: [4, 32, 2048]

        # 3. Expertë³„ ì²˜ë¦¬
        if adarms_cond[i] is None:  # Expert 0 (Prefix)
            # â”€â”€â”€ Regular RMSNorm â”€â”€â”€
            scale = self.param("scale", zeros_init(), (2048,))  # [2048]
            x_norm = normed_inputs * (1 + scale)
            gate = None

        else:  # Expert 1 (Suffix) - Ï€â‚€.â‚…
            # â”€â”€â”€ Adaptive RMSNorm (AdaRMS) â”€â”€â”€
            # Modulation network
            modulation = nn.Dense(2048 * 3)(adarms_cond[i])
            # Input: [4, 2048] â†’ Output: [4, 6144]

            # Split into scale, shift, gate
            scale, shift, gate = jnp.split(modulation, 3, axis=-1)
            # scale: [4, 2048]
            # shift: [4, 2048]
            # gate:  [4, 2048]

            # AdaIN (Adaptive Instance Normalization) style
            scale_expanded = scale[:, None, :]  # [4, 1, 2048]
            shift_expanded = shift[:, None, :]  # [4, 1, 2048]

            x_norm = normed_inputs * (1 + scale_expanded) + shift_expanded
            # [4, 32, 2048] * [4, 1, 2048] + [4, 1, 2048]
            # â†’ [4, 32, 2048]

        pre_attn.append(x_norm)
        gates.append(gate)

# âœ… Output:
# pre_attn[0]: [4, 784, 2048]  (Normalized prefix, no conditioning)
# pre_attn[1]: [4, 32, 2048]   (Normalized suffix, with timestep conditioning)
# gates[0]: None
# gates[1]: [4, 2048] (for gated residual later)
```

### Step 7-2: QKV Projection (Multi-Expert)

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:158-199`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-2: QKV Projection (Expert-specific Weights)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

qkvs = []

for i, (x, config) in enumerate(zip(pre_attn, configs)):
    if x is None:
        continue

    # â”€â”€â”€ Grouped Query Attention (GQA) â”€â”€â”€
    # num_heads = 8, num_kv_heads = 1, head_dim = 256

    # Query Projection
    q_einsum = lora.Einsum(
        shape=(8, 2048, 256),  # (num_heads, width, head_dim)
        name=_name("q_einsum", i),  # "q_einsum" or "q_einsum_1"
        ...
    )
    q = q_einsum("BTD,NDH->BTNH", x)
    # x[0]: [4, 784, 2048] â†’ q[0]: [4, 784, 8, 256]
    # x[1]: [4, 32, 2048]  â†’ q[1]: [4, 32, 8, 256]

    # Key/Value Projection (shared, only 1 head for GQA)
    kv_einsum = lora.Einsum(
        shape=(2, 1, 2048, 256),  # (2, num_kv_heads, width, head_dim)
        name=_name("kv_einsum", i),  # "kv_einsum" or "kv_einsum_1"
        ...
    )
    k, v = kv_einsum("BSD,2KDH->2BSKH", x)
    # x[0]: [4, 784, 2048] â†’ k[0], v[0]: [4, 784, 1, 256]
    # x[1]: [4, 32, 2048]  â†’ k[1], v[1]: [4, 32, 1, 256]

    qkvs.append((q, k, v))

# âœ… Output:
# qkvs[0]: (q[4,784,8,256], k[4,784,1,256], v[4,784,1,256]) â† Expert 0 weight
# qkvs[1]: (q[4,32,8,256],  k[4,32,1,256],  v[4,32,1,256])  â† Expert 1 weight
#          ^^^ ì™„ì „íˆ ë…ë¦½ì ì¸ weight ì‚¬ìš©!
```

### Step 7-3: RoPE (Rotary Position Embedding)

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:424-440`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-3: Apply RoPE to Q and K
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Concatenate QKV from all experts
q, k, v = (jnp.concatenate(y, axis=1) for y in zip(*qkvs))
# q: [4, 816, 8, 256]  (784 + 32)
# k: [4, 816, 1, 256]
# v: [4, 816, 1, 256]

def _apply_rope(x, positions, max_wavelength=10_000):
    # x: [4, 816, H, 256]
    # positions: [4, 816]

    # Frequency ê³„ì‚°
    freq_exponents = (2.0 / 256) * jnp.arange(256 // 2)  # [128]
    timescale = max_wavelength ** freq_exponents
    # timescale: [10000^0, ..., 10000^(254/256)]

    # Positionì— ë”°ë¥¸ radians
    radians = positions[..., None] / timescale[None, None, :]
    # [4, 816, 1] / [1, 1, 128] = [4, 816, 128]
    radians = radians[..., None, :]  # [4, 816, 1, 128]

    # Sin/Cos ê³„ì‚°
    sin, cos = jnp.sin(radians), jnp.cos(radians)
    # sin, cos: [4, 816, 1, 128]

    # Split features into two halves
    x1, x2 = jnp.split(x, 2, axis=-1)
    # x1, x2: [4, 816, H, 128]

    # Rotation
    res = jnp.concatenate([
        x1 * cos - x2 * sin,  # [4, 816, H, 128]
        x2 * cos + x1 * sin,  # [4, 816, H, 128]
    ], axis=-1)  # [4, 816, H, 256]

    return res

# Apply RoPE
q = _apply_rope(q, positions=positions)  # [4, 816, 8, 256]
k = _apply_rope(k, positions=positions)  # [4, 816, 1, 256]

# Scale Q by 1/âˆšhead_dim
q *= 256 ** -0.5  # â‰ˆ 0.0625

# âœ… Output:
# q: [4, 816, 8, 256] (with positional info)
# k: [4, 816, 1, 256] (with positional info)
# v: [4, 816, 1, 256] (no change)
```

### Step 7-4: Grouped Query Attention

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:216-231`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-4: Grouped Query Attention (GQA)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Reshape Q for Grouped Query Attention
q = einops.rearrange(q, "B T (K G) H -> B T K G H", K=1)
# [4, 816, 8, 256] â†’ [4, 816, 1, 8, 256]
#  B   T   N   H       B   T   K  G  H
# N = K Ã— G (num_heads = num_kv_heads Ã— group_size)
# 8 = 1 Ã— 8

# Attention scores
logits = jnp.einsum("BTKGH,BSKH->BKGTS", q, k)
# q: [4, 816, 1, 8, 256]
# k: [4, 816, 1, 256]
# logits: [4, 1, 8, 816, 816]
#         B  K  G   T    S

# Apply attention mask
big_neg = -2.3819763e38  # Large negative value
attn_mask_expanded = attn_mask[:, None, None, :, :]
# [4, 816, 816] â†’ [4, 1, 1, 816, 816]

masked_logits = jnp.where(attn_mask_expanded, logits, big_neg)
# masked_logits: [4, 1, 8, 816, 816]

# Softmax (in float32 for stability)
probs = jax.nn.softmax(masked_logits, axis=-1).astype(dtype)
# probs: [4, 1, 8, 816, 816]

# Apply to values
encoded = jnp.einsum("BKGTS,BSKH->BTKGH", probs, v)
# probs: [4, 1, 8, 816, 816]
# v: [4, 816, 1, 256]
# encoded: [4, 816, 1, 8, 256]

# Reshape back
encoded = einops.rearrange(encoded, "B T K G H -> B T (K G) H")
# [4, 816, 1, 8, 256] â†’ [4, 816, 8, 256]

# âœ… Output:
# encoded: [4, 816, 8, 256]  (attention-weighted values)
```

### Step 7-5: Output Projection (Multi-Expert)

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:233-249`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-5: Output Projection (Expertë³„ ë…ë¦½)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# encoded: [4, 816, 8, 256] (ëª¨ë“  í† í°ì˜ attention output)

out = []
start = 0

for i, (x, config) in enumerate(zip(xs, configs)):
    if x is not None:
        end = start + x.shape[1]
        # Expert 0: start=0, end=784
        # Expert 1: start=784, end=816

        # Expertë³„ ë…ë¦½ì ì¸ output projection
        out_einsum = lora.Einsum(
            shape=(8, 256, 2048),  # (num_heads, head_dim, width)
            name=_name("attn_vec_einsum", i),  # "attn_vec_einsum" or "_1"
            ...
        )

        # Slice and project
        expert_encoded = encoded[:, start:end]
        # Expert 0: [4, 784, 8, 256]
        # Expert 1: [4, 32, 8, 256]

        expert_out = out_einsum("BTNH,NHD->BTD", expert_encoded)
        # Expert 0: [4, 784, 2048]
        # Expert 1: [4, 32, 2048]

        out.append(expert_out)
        start = end
    else:
        out.append(None)

# âœ… Output:
# out[0]: [4, 784, 2048]  (Prefix, Expert 0 weight ì‚¬ìš©)
# out[1]: [4, 32, 2048]   (Suffix, Expert 1 weight ì‚¬ìš©)
```

### Step 7-6: Gated Residual Connection

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:309-312` + `453-459`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-6: Gated Residual (AdaRMS gate)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _gated_residual(x, y, gate):
    if x is None or y is None:
        return None
    if gate is None:
        return x + y  # Regular residual
    return x + y * gate  # Gated residual

xs = [
    _gated_residual(xs[0], out[0], gates[0]),
    _gated_residual(xs[1], out[1], gates[1]),
]

# Expert 0 (Prefix):
# xs[0] = prefix_tokens + out[0]
# [4, 784, 2048] + [4, 784, 2048] = [4, 784, 2048]

# Expert 1 (Suffix) - with AdaRMS gate:
# xs[1] = suffix_tokens + out[1] * gates[1]
# [4, 32, 2048] + [4, 32, 2048] * [4, 1, 2048] = [4, 32, 2048]
#                                   ^^^^^^^
#                                   Gate controls residual strength

# âœ… Output:
# xs[0]: [4, 784, 2048]  (after first residual)
# xs[1]: [4, 32, 2048]   (after gated residual)
```

### Step 7-7: FeedForward Network

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:314-330`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-7: FeedForward Network (Expertë³„ ë…ë¦½)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

out = []
gates = []

for i, (x, config) in enumerate(zip(xs, configs)):
    if x is not None:
        # â”€â”€â”€ Pre-FFN RMSNorm â”€â”€â”€
        x_norm, gate = RMSNorm(name=_name("pre_ffw_norm", i))(
            x, adarms_cond[i]
        )
        # Same AdaRMS logic as before
        # x_norm: [4, 784, 2048] or [4, 32, 2048]
        # gate: None or [4, 2048]

        # â”€â”€â”€ FeedForward â”€â”€â”€
        # Gated FFN (SwiGLU variant)
        w_gating = self.param(
            _name("gating_einsum", i),
            ...,
            (2, 2048, 16384)  # (2, features, mlp_dim)
        )

        # Two projections
        ff_gate = jnp.dot(x_norm, w_gating[0])  # [B, T, 16384]
        gate_value = nn.gelu(ff_gate)

        ff1 = jnp.dot(x_norm, w_gating[1])  # [B, T, 16384]
        activations = gate_value * ff1  # Element-wise multiply

        # Output projection
        w_linear = self.param(
            _name("linear", i),
            ...,
            (16384, 2048)
        )
        outputs = jnp.dot(activations, w_linear)  # [B, T, 2048]

        out.append(outputs)
        gates.append(gate)

# â”€â”€â”€ Second Gated Residual â”€â”€â”€
xs = [
    _gated_residual(xs[0], out[0], gates[0]),
    _gated_residual(xs[1], out[1], gates[1]),
]

# âœ… Output (Layer 0 ì™„ë£Œ):
# xs[0]: [4, 784, 2048]  (Prefix after full transformer block)
# xs[1]: [4, 32, 2048]   (Suffix after full transformer block)
```

---

## ğŸ“ Step 8: Transformer Layers 1-17

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:365-381` (nn.scan)

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 8: Repeat Layer 0 for Layers 1-17
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# nn.scanì„ ì‚¬ìš©í•˜ì—¬ 18ê°œ layerë¥¼ ìë™ìœ¼ë¡œ ë°˜ë³µ
# ê° layerëŠ” ë™ì¼í•œ êµ¬ì¡°, ë‹¤ë¥¸ weight

for layer_idx in range(1, 18):
    # Layer 0ê³¼ ë™ì¼í•œ ê³¼ì • ë°˜ë³µ:
    # 1. Pre-Attention RMSNorm (with AdaRMS)
    # 2. QKV Projection (Expert-specific)
    # 3. RoPE
    # 4. Grouped Query Attention
    # 5. Output Projection (Expert-specific)
    # 6. Gated Residual
    # 7. Pre-FFN RMSNorm (with AdaRMS)
    # 8. FeedForward (Expert-specific)
    # 9. Gated Residual

    pass  # Automatically handled by nn.scan

# âœ… After 18 layers:
# xs[0]: [4, 784, 2048]  (Prefix, fully processed)
# xs[1]: [4, 32, 2048]   (Suffix, fully processed)
```

---

## ğŸ“ Step 9: Final Layer Normalization

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:409-411`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 9: Final RMSNorm (Expertë³„)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# xs[0]: [4, 784, 2048]
# xs[1]: [4, 32, 2048]

outputs = []
for i, (x, final_norm) in enumerate(zip(xs, self.final_norms)):
    if x is not None:
        # Final RMSNorm (no AdaRMS here)
        x_final, _ = final_norm(x, adarms_cond[i])
        outputs.append(x_final)
    else:
        outputs.append(None)

# âœ… Output:
# outputs[0]: [4, 784, 2048]  (Prefix final output)
# outputs[1]: [4, 32, 2048]   (Suffix final output)
```

---

## ğŸ“ Step 10: Velocity Prediction

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:212`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 10: Action Tokens â†’ Velocity Prediction
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# suffix_out: [4, 32, 2048]

# Only take the last action_horizon tokens
action_output = suffix_out[:, -32:]  # [4, 32, 2048]

# Project to action dimension
v_t = self.action_out_proj(action_output)
# Linear(2048 â†’ 7)
# v_t: [4, 32, 7]

# âœ… Output:
# v_t: [4, 32, 7]  (Predicted velocity field)
```

---

## ğŸ“ Step 11: Flow Matching Loss

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:214`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 11: Compute Flow Matching Loss
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€ Target velocity â”€â”€â”€
u_t = noise - actions
# [4, 32, 7] - [4, 32, 7] = [4, 32, 7]

# â”€â”€â”€ Loss â”€â”€â”€
loss = jnp.mean(jnp.square(v_t - u_t), axis=-1)
# MSE loss averaged over action dimensions
# loss: [4, 32]  (loss per timestep)

# Average over timesteps
final_loss = jnp.mean(loss)
# final_loss: scalar

# âœ… Output:
# final_loss: scalar (training objective)
```

---

## ğŸ“ Inference: Flow Matching Sampling

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:217-279`

Inference ì‹œì—ëŠ” noiseì—ì„œ ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ denoisingí•©ë‹ˆë‹¤.

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Inference: Iterative Denoising (Euler Integration)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def sample_actions(self, rng, observation, num_steps=10):
    # â”€â”€â”€ Step I-1: Prefix KV Cache ìƒì„± (í•œ ë²ˆë§Œ) â”€â”€â”€
    prefix_tokens, prefix_mask, prefix_ar_mask = self.embed_prefix(observation)
    prefix_attn_mask = make_attn_mask(prefix_mask, prefix_ar_mask)
    positions = jnp.cumsum(prefix_mask, axis=1) - 1

    _, kv_cache = self.PaliGemma.llm(
        [prefix_tokens, None],  # Expert 0ë§Œ ì²˜ë¦¬
        mask=prefix_attn_mask,
        positions=positions,
    )
    # kv_cache: [18, 4, 784, 1, 256]  â† ì €ì¥!

    # â”€â”€â”€ Step I-2: ì´ˆê¸°í™” â”€â”€â”€
    noise = jax.random.normal(rng, (4, 32, 7))
    x_t = noise  # time=1.0ì—ì„œ ì‹œì‘ (pure noise)
    dt = -1.0 / num_steps  # -0.1

    # â”€â”€â”€ Step I-3: Iterative Denoising â”€â”€â”€
    def step(carry):
        x_t, time = carry
        # time: 1.0 â†’ 0.9 â†’ 0.8 â†’ ... â†’ 0.1 â†’ 0.0

        # Suffix embedding
        suffix_tokens, suffix_mask, suffix_ar_mask, adarms_cond = \
            self.embed_suffix(observation, x_t, jnp.broadcast_to(time, [4]))

        # Attention mask
        suffix_attn_mask = make_attn_mask(suffix_mask, suffix_ar_mask)
        prefix_attn_mask = einops.repeat(prefix_mask, "b p -> b s p", s=32)
        full_attn_mask = jnp.concatenate([prefix_attn_mask, suffix_attn_mask], axis=-1)

        # Positions
        positions = jnp.sum(prefix_mask, axis=-1)[:, None] + jnp.cumsum(suffix_mask, axis=-1) - 1

        # Transformer (Expert 1ë§Œ, KV cache ì¬ì‚¬ìš©!)
        (prefix_out, suffix_out), _ = self.PaliGemma.llm(
            [None, suffix_tokens],  # PrefixëŠ” None (cacheì—ì„œ ê°€ì ¸ì˜´)
            mask=full_attn_mask,
            positions=positions,
            kv_cache=kv_cache,  # â† ì €ì¥ëœ cache ì¬ì‚¬ìš©!
            adarms_cond=[None, adarms_cond],
        )

        # Velocity ì˜ˆì¸¡
        v_t = self.action_out_proj(suffix_out[:, -32:])

        # Euler integration: x_{t+dt} = x_t + dt * v_t
        x_t = x_t + dt * v_t
        time = time + dt

        return x_t, time

    def cond(carry):
        x_t, time = carry
        return time >= -dt / 2  # time > 0

    # While loop (10 iterations)
    x_0, _ = jax.lax.while_loop(cond, step, (noise, 1.0))

    return x_0  # [4, 32, 7]  â† Denoised actions!

# âœ… Inference ê²°ê³¼:
# x_0: [4, 32, 7]  (Clean actions)
```

---

## ğŸ“Š ì „ì²´ ë°ì´í„° íë¦„ ìš”ì•½

### Shape ë³€í™” ì¶”ì 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Input Data                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Images:  3 Ã— [4, 224, 224, 3]  uint8 [0, 255]             â”‚
â”‚ State:   [4, 7]                float32                     â”‚
â”‚ Text:    [4, 16]               int32                       â”‚
â”‚ Actions: [4, 32, 7]            float32 (training)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Step 1: Preprocessing                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Images â†’ float32 [-1, 1]                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Step 2-4: Prefix Embedding                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Images:  3 Ã— [4, 256, 2048]    (SigLIP)                   â”‚
â”‚ Text:    [4, 16, 2048]         (Embedder)                 â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚ Prefix:  [4, 784, 2048]        (Concatenated)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Step 5: Suffix Embedding                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Actions: [4, 32, 7] â†’ [4, 32, 2048]  (Linear projection)  â”‚
â”‚ Time:    [4] â†’ [4, 2048]             (Sinusoidal + MLP)   â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚ Suffix:  [4, 32, 2048]                                     â”‚
â”‚ AdaRMS:  [4, 2048]                   (conditioning)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Step 6: Attention Mask Generation                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mask:     [4, 816, 816]         (Prefix-LM + Causal)      â”‚
â”‚ Positions: [4, 816]                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Step 7-8: Multi-Expert Transformer (18 layers)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ xs[0]: [4, 784, 2048] â”€â”€â†’ ... â”€â”€â†’ [4, 784, 2048]          â”‚
â”‚        (Expert 0 weights)                                   â”‚
â”‚                                                             â”‚
â”‚ xs[1]: [4, 32, 2048]  â”€â”€â†’ ... â”€â”€â†’ [4, 32, 2048]           â”‚
â”‚        (Expert 1 weights, AdaRMS conditioning)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Step 9: Final Normalization                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Prefix:  [4, 784, 2048]                                    â”‚
â”‚ Suffix:  [4, 32, 2048]                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Step 10: Velocity Prediction                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ v_t: [4, 32, 7]                 (Predicted velocity)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Step 11: Loss Computation                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ loss = mean_squared_error(v_t, u_t)                        â”‚
â”‚      = mean((v_t - (noise - actions))^2)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¥ í•µì‹¬ í¬ì¸íŠ¸

### 1. Multi-Expert ë©”ì»¤ë‹ˆì¦˜

```python
# ê° expertëŠ” ìì‹ ë§Œì˜ weight ì‚¬ìš©:

# Expert 0 (Prefix - Image/Text):
# - "q_einsum": [8, 2048, 256]
# - "kv_einsum": [2, 1, 2048, 256]
# - "attn_vec_einsum": [8, 256, 2048]
# - "mlp/gating_einsum": [2, 2048, 16384]
# - "mlp/linear": [16384, 2048]

# Expert 1 (Suffix - Action):
# - "q_einsum_1": [8, 2048, 256]     â† ë‹¤ë¥¸ weight!
# - "kv_einsum_1": [2, 1, 2048, 256]
# - "attn_vec_einsum_1": [8, 256, 2048]
# - "mlp/gating_einsum_1": [2, 2048, 16384]
# - "mlp/linear_1": [16384, 2048]
```

### 2. AdaRMS Conditioning (Ï€â‚€.â‚…)

```python
# Timestepì„ adaptive normalizationìœ¼ë¡œ ì£¼ì…:

time_emb = posemb_sincos(time)  # [4, 2048]

# RMSNormì—ì„œ:
modulation = Dense(2048 * 3)(time_emb)  # [4, 6144]
scale, shift, gate = split(modulation, 3)  # ê° [4, 2048]

normed = normed * (1 + scale[:, None, :]) + shift[:, None, :]
# Timestepì— ë”°ë¼ normalization íŒŒë¼ë¯¸í„° ë³€ê²½!

# Gated residual:
x = x + y * gate[:, None, :]
# Timestepì— ë”°ë¼ residual ê°•ë„ ì¡°ì ˆ!
```

### 3. Flow Matching

```python
# Training:
time ~ Beta(1.5, 1.0)  # [0, 1]
x_t = time * noise + (1 - time) * actions
u_t = noise - actions  # Target velocity
loss = ||v_t - u_t||^2

# Inference (Euler integration):
x_t = noise  # t=1
for t in [1.0, 0.9, 0.8, ..., 0.1, 0.0]:
    v_t = model(x_t, t)
    x_t = x_t - 0.1 * v_t  # Euler step
# x_0 = clean actions
```

### 4. Attention Pattern

```
Prefix (Image/Text):
  â”œâ”€ Bidirectional attention
  â””â”€ Can attend to everything

Suffix (Action):
  â”œâ”€ Can attend to Prefix
  â””â”€ Causal attention within Suffix
```

### 5. KV Cache Reuse (Inference)

```python
# Prefixë¥¼ í•œ ë²ˆë§Œ ì²˜ë¦¬:
_, kv_cache = llm([prefix_tokens, None], ...)
# kv_cache: [18 layers, 4, 784, 1, 256]

# 10ë²ˆ ë°˜ë³µí•  ë•Œë§ˆë‹¤ ì¬ì‚¬ìš©:
for step in range(10):
    _, _ = llm([None, suffix_tokens], kv_cache=kv_cache, ...)
    # PrefixëŠ” ì¬ê³„ì‚° ì•ˆ í•¨! â† 10ë°° ë¹ ë¦„
```

---

## ğŸ“ ë³€ê²½ ì´ë ¥

- 2026-02-08: ì´ˆì•ˆ ì‘ì„±
  - ì „ì²´ ë°ì´í„° íë¦„ Step-by-Step ì •ë¦¬
  - ê° ë‹¨ê³„ë³„ ìƒì„¸ ì½”ë“œ ì„¤ëª…
  - Shape ë³€í™” ì¶”ì 
  - í•µì‹¬ í¬ì¸íŠ¸ ì •ë¦¬

---

**ì‘ì„±ì**: AI Analysis
**í”„ë¡œì íŠ¸**: openpi (Physical Intelligence)
**ë²„ì „**: 1.0
