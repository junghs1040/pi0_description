# Ï€â‚€.â‚… ëª¨ë¸ ë°ì´í„° íë¦„ Step-by-Step ì™„ì „ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” Ï€â‚€.â‚… ëª¨ë¸ì—ì„œ **ì…ë ¥ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ì–´ ìµœì¢… ì¶œë ¥ì´ ë˜ëŠ”ì§€**ë¥¼ í•œ ë‹¨ê³„ì”© ì¶”ì í•©ë‹ˆë‹¤.
  
> **ğŸ“Œ Ï€â‚€ vs Ï€â‚€.â‚… í•µì‹¬ ì°¨ì´ì **
>
> | | Ï€â‚€ | Ï€â‚€.â‚… |
> |---|---|---|
> | **State ì…ë ¥ ë°©ì‹** | continuous suffix í† í° (1ê°œ) | ì´ì‚° ì–¸ì–´ í† í°ìœ¼ë¡œ í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ prefixì— í¬í•¨ |
> | **Timestep ì£¼ì…** | Action + Time concat â†’ MLP (suffix ë‚´ë¶€) | **AdaRMSNorm** â€” Action Expertì˜ ëª¨ë“  layerì— ì¡°ê±´ ì£¼ì… |
> | **Suffix êµ¬ì„±** | `[state(1), action(32)]` = 33 tokens | `[action(50)]` = 50 tokens (state ì—†ìŒ) |
> | **max_token_len** | 48 | 200 (stateê°€ í…ìŠ¤íŠ¸ë¡œ ë“¤ì–´ê°€ë¯€ë¡œ ë” ê¸¸ì–´ì§) |
> | **action_horizon** | 50 (ê¸°ë³¸ê°’ ë™ì¼) | 50 |
> | **action_dim** | 32 (ê¸°ë³¸ê°’ ë™ì¼) | 32 |

> **ğŸ“Œ í•™ìŠµ vs ì¶”ë¡  êµ¬ë¶„**
> - ğŸ‹ï¸ **í•™ìŠµ**: Ground truth actions + noise â†’ Flow Matching loss ê³„ì‚°
> - ğŸ¯ **ì¶”ë¡ **: Pure noiseì—ì„œ ì‹œì‘ â†’ 10íšŒ Euler integrationìœ¼ë¡œ action ìƒì„±

Step 0: ì›ë³¸ ì…ë ¥ ë°ì´í„° (Images, State-as-text, Actions)  
Step 1: Observation ê°ì²´ ìƒì„± (State â†’ í† í°í™”, uint8 â†’ float32 ì •ê·œí™”)  
Step 2: Image Embedding (SigLIP) - 3Ã—256 = 768 tokens    
Step 3: State+Text Embedding (Gemma Embedder) - ìµœëŒ€ 200 tokens  
Step 4: Prefix Concatenation - ìµœëŒ€ 968 tokens (Image + State/Text)  
Step 5: Action Embedding (Suffix) - 50 tokens + AdaRMSNorm ì¡°ê±´ ìƒì„±  
Step 6: Attention Mask ìƒì„± - [4, 1018, 1018]  
Step 7: Transformer Layer 0 ìƒì„¸ ë¶„ì„ (AdaRMSNorm í¬í•¨)  
7-1: Pre-Attention AdaRMSNorm (Adaptive, time_emb ì¡°ê±´ë¶€)  
7-2: QKV Projection (Multi-Expert)  
7-3: RoPE (Rotary Position Embedding)  
7-4: Grouped Query Attention  
7-5: Output Projection (Expertë³„)  
7-6: Gated Residual Connection (AdaRMSNorm gate)  
7-7: FeedForward Network (AdaRMSNorm gate ì ìš©)    
Step 8: Transformer Layers 1-17 (18 layers total)  
Step 9: Final Layer Normalization (AdaRMSNorm)  
Step 10-11: Velocity Prediction + Flow Matching Loss  


**ì˜ˆì‹œ ë°ì´í„°**:
- Batch Size: B = 4  
- Images: 3ê°œ (base_0, left_wrist_0, right_wrist_0)  
- Text + State: ìµœëŒ€ 200 tokens (stateê°€ í…ìŠ¤íŠ¸ í† í°ìœ¼ë¡œ ì¸ì½”ë”©ë¨)  
- Actions: 50 timesteps, 32 DoF  
- Model: Ï€â‚€.â‚… (`pi05=True`)  

---

## ğŸ“ Step 0: ì›ë³¸ ì…ë ¥ ë°ì´í„°

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 0: Raw Input (Python Dictionary) â€” Ï€â‚€.â‚…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

raw_input = {
    # â”€â”€â”€ Images â”€â”€â”€
    "image": {
        "base_0_rgb":        np.array([4, 224, 224, 3], dtype=uint8),
        "left_wrist_0_rgb":  np.array([4, 224, 224, 3], dtype=uint8),
        "right_wrist_0_rgb": np.array([4, 224, 224, 3], dtype=uint8),
    },
    "image_mask": {
        "base_0_rgb":        np.array([True, True, True, True]),
        "left_wrist_0_rgb":  np.array([True, True, True, True]),
        "right_wrist_0_rgb": np.array([True, True, True, True]),
    },

    # â”€â”€â”€ Robot State (Ï€â‚€.â‚…ì—ì„œëŠ” í…ìŠ¤íŠ¸ í† í°ìœ¼ë¡œ ë³€í™˜ë¨) â”€â”€â”€
    # ì˜ˆ: "state: [0.12, -0.34, ..., 0.56]" í˜•íƒœì˜ ë¬¸ìì—´ì´
    # ì–¸ì–´ ëª…ë ¹ promptì™€ í•¨ê»˜ tokenized_prompt ì— í•©ì‚°ë¨
    # ì•„ë˜ëŠ” ì´ë¯¸ policy_config.py ì˜ ì „ì²˜ë¦¬ í›„ ìƒíƒœ

    # â”€â”€â”€ Tokenized Prompt (State + Language, í•©ì³ì„œ í…ìŠ¤íŠ¸í™”) â”€â”€â”€
    "tokenized_prompt": np.array([
        [15234, 67, 123, ..., 8821, 0, 0, ...],  # "state: [0.12, ...] pick up fork" + padding
        ...
    ], dtype=int32),  # [4, 200]
    "tokenized_prompt_mask": np.array([
        [True, True, ..., True, False, ...],  # ì‹¤ì œ í† í°ì€ True, paddingì€ False
        ...
    ], dtype=bool),  # [4, 200]

    # â”€â”€â”€ Actions (Training only) â”€â”€â”€
    "actions": np.array([4, 50, 32], dtype=float32),  # Ground truth actions
}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 0 ìš”ì•½ (Ï€â‚€.â‚… í•µì‹¬ ì°¨ì´):
  Ï€â‚€.â‚…ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë³€í™”: Stateê°€ ë” ì´ìƒ ë³„ë„ suffix í† í°ì´ ì•„ë‹˜.
  - StateëŠ” í…ìŠ¤íŠ¸ë¡œ ì§ë ¬í™”ë˜ì–´ language prompt ë’¤ì— ë¶™ì–´
    í•˜ë‚˜ì˜ tokenized_promptë¡œ í•©ì³ì§„ë‹¤.
  - ì˜ˆ: "pick up the cup\nstate: 0.12, -0.34, 0.56, ..."
  - ì´ë¡œ ì¸í•´ max_token_lenì´ 48 â†’ 200ìœ¼ë¡œ ì¦ê°€
  - Actions: 50 timesteps, 32 DoF (Ï€â‚€ë³´ë‹¤ horizonì´ ê¸¸ê³  DoFê°€ ë§ìŒ)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

> **ğŸ”„ í•™ìŠµ vs ì¶”ë¡ **
> | | í•™ìŠµ (Training) | ì¶”ë¡  (Inference) |
> |---|---|---|
> | **Images** | ë™ì¼ | ë™ì¼ |
> | **tokenized_prompt** | ë™ì¼ | ë™ì¼ |
> | **Actions** | âœ… Ground truth í•„ìš” | âŒ ì—†ìŒ (noiseì—ì„œ ìƒì„±) |

---

## ğŸ“ Step 1: Observation ê°ì²´ ìƒì„±

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/model.py` + `src/openpi/policies/policy_config.py`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 1: Dictionary â†’ Observation Object (Ï€â‚€.â‚…)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

observation = Observation.from_dict(raw_input)

# â”€â”€â”€ ë‚´ë¶€ ì²˜ë¦¬ â”€â”€â”€
# 1. uint8 ì´ë¯¸ì§€ë¥¼ float32 [-1, 1]ë¡œ ì •ê·œí™”
for key in raw_input["image"]:
    image = raw_input["image"][key]  # [4, 224, 224, 3] uint8 [0, 255]
    image = image.astype(float32) / 255.0 * 2.0 - 1.0  # float32 [-1, 1]

# 2. êµ¬ì¡°í™”ëœ Observation ê°ì²´ ìƒì„±
observation = Observation(
    images={
        "base_0_rgb":        [4, 224, 224, 3],  # float32, [-1, 1]
        "left_wrist_0_rgb":  [4, 224, 224, 3],
        "right_wrist_0_rgb": [4, 224, 224, 3],
    },
    image_masks={
        "base_0_rgb":        [4],  # bool
        "left_wrist_0_rgb":  [4],
        "right_wrist_0_rgb": [4],
    },
    state=[4, 32],                 # float32 (ë‚´ë¶€ ì°¸ì¡°ìš©, suffixì—ëŠ” ì‚¬ìš© ì•ˆ í•¨)
    tokenized_prompt=[4, 200],     # int32  â† Ï€â‚€(16) ë³´ë‹¤ í›¨ì”¬ ê¹€
    tokenized_prompt_mask=[4, 200] # bool
)

# âœ… Output Shape:
# - Images: 3ê°œ Ã— [4, 224, 224, 3] float32 [-1, 1]
# - State:  [4, 32] float32  (Ï€â‚€.â‚…ì—ì„œëŠ” prefixì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ suffixì— ë³„ë„ ì£¼ì… ì—†ìŒ)
# - tokenized_prompt: [4, 200] int32

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 1 ìš”ì•½ (Ï€â‚€ vs Ï€â‚€.â‚…):
  Ï€â‚€:   state = [4, 7]  â†’ suffixì—ì„œ Linear(7â†’1024)ë¡œ 1ê°œ state í† í° ìƒì„±
  Ï€â‚€.â‚…: state = [4, 32] â†’ í…ìŠ¤íŠ¸ë¡œ ì§ë ¬í™”ë˜ì–´ tokenized_prompt[4, 200]ì— ì´ë¯¸ í¬í•¨
        ë”°ë¼ì„œ suffix ìƒì„± ì‹œ state í† í°ì„ ë³„ë„ë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ Step 2: Image Embedding (SigLIP)

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:113-125` + `src/openpi/models/siglip.py`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 2: Images â†’ Image Tokens (SigLIP Vision Encoder)
# Ï€â‚€.â‚…ëŠ” Ï€â‚€ì™€ ì™„ì „íˆ ë™ì¼í•œ SigLIP êµ¬ì¡° ì‚¬ìš©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

image_tokens_list = []
for image_name in observation.images:
    image = observation.images[image_name]  # [4, 224, 224, 3]

    image_tokens, _ = self.PaliGemma.img(image, train=False)

    # â”€â”€â”€ SigLIP ë‚´ë¶€ ì²˜ë¦¬ (Ï€â‚€ì™€ ë™ì¼) â”€â”€â”€
    # 2-1. Patch Embedding: [4, 224, 224, 3] â†’ patches [4, 256, 588]
    # 2-2. Positional Embedding (Sinusoidal 2D)
    # 2-3. Transformer Encoder (27 layers, So400m/14)
    #       - width: 1152, heads: 16, head_dim: 72
    # 2-4. Final Projection: nn.Dense(1152 â†’ 2048)

    image_tokens_list.append(image_tokens)  # [4, 256, 2048]

# âœ… Output:
# image_tokens_list = [
#     [4, 256, 2048],  # base_0_rgb
#     [4, 256, 2048],  # left_wrist_0_rgb
#     [4, 256, 2048],  # right_wrist_0_rgb
# ]
# Total: 3 Ã— 256 = 768 image tokens

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 2 ìš”ì•½:
  Ï€â‚€.â‚…ë„ Ï€â‚€ì™€ ì™„ì „íˆ ë™ì¼í•œ SigLIP (ViT-So400m/14) ì‚¬ìš©.
  - 224Ã—224 ì´ë¯¸ì§€ â†’ 14Ã—14 í¬ê¸° íŒ¨ì¹˜ 256ê°œ â†’ 27ì¸µ ViT â†’ 2048ì°¨ì›
  - 3ê°œ ì¹´ë©”ë¼ ê°ê° ë…ë¦½ ì²˜ë¦¬ â†’ ì´ 768ê°œ ì´ë¯¸ì§€ í† í°
  - ì°¨ì´ ì—†ìŒ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ Step 3: Text/State Embedding (Gemma Embedder)

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:148-154` + `pi0.py:128-133`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 3: Token IDs â†’ Text Embeddings (Ï€â‚€.â‚…)
# Ï€â‚€.â‚…ëŠ” tokenized_prompt ê°€ state ì •ë³´ë¥¼ ì´ë¯¸ í¬í•¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì…ë ¥: observation.tokenized_prompt
# [4, 200] int32  â† Ï€â‚€ëŠ” [4, 16] ì´ì—ˆìŒ
tokenized_prompt = observation.tokenized_prompt  # [4, 200]

tokenized_inputs = self.PaliGemma.llm(tokenized_prompt, method="embed")

# â”€â”€â”€ Embedder ë‚´ë¶€ â”€â”€â”€
# 1. Embedding table lookup: [257152, 2048]
# x = embedding_table[(tokenized_prompt,)]  # [4, 200, 2048]
# 2. Scale: x *= âˆš2048 â‰ˆ 45.25

# âœ… Output:
# text_tokens: [4, 200, 2048]
#
# ë‚´ìš© ì˜ˆì‹œ (Ï€â‚€.â‚…):
# tokens[0:5]   â†’ "pick up the fork" ì–¸ì–´ ëª…ë ¹
# tokens[5:...]  â†’ "state: 0.12, -0.34, 0.56, ..." ìƒíƒœ í…ìŠ¤íŠ¸
# tokens[-1~]   â†’ padding

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 3 ìš”ì•½ (Ï€â‚€ vs Ï€â‚€.â‚…):
  Ï€â‚€:   text_tokens: [4, 16, 2048]   (ì–¸ì–´ ëª…ë ¹ë§Œ)
  Ï€â‚€.â‚…: text_tokens: [4, 200, 2048]  (ì–¸ì–´ ëª…ë ¹ + state í…ìŠ¤íŠ¸ ì§ë ¬í™”)

  Stateë¥¼ í…ìŠ¤íŠ¸ë¡œ í‘œí˜„í•˜ë©´:
  - ì¥ì : ì—°ì†ê°’ì„ ì–¸ì–´ ëª¨ë¸ì˜ ê°•ë ¥í•œ í‘œí˜„ë ¥ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥
  - ì¥ì : state ì°¨ì›ì´ ë‹¬ë¼ë„ ë²”ìš©ì ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥ (ë‹¤ì–‘í•œ ë¡œë´‡ ì§€ì›)
  - ë‹¨ì : í† í° ìˆ˜ê°€ ëŠ˜ì–´ë‚¨ (16 â†’ 200)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ Step 4: Prefix Token Concatenation

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:106-137`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 4: Image + Text/State â†’ Prefix Sequence (Ï€â‚€.â‚…)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def embed_prefix(self, obs):
    tokens = []
    input_mask = []
    ar_mask = []

    # â”€â”€â”€ 4-1: Image tokens ì¶”ê°€ (Ï€â‚€ì™€ ë™ì¼) â”€â”€â”€
    for name in obs.images:
        image_tokens = image_tokens_list.pop(0)  # [4, 256, 2048]
        tokens.append(image_tokens)
        mask = einops.repeat(obs.image_masks[name], "b -> b s", s=256)  # [4, 256]
        input_mask.append(mask)
        ar_mask += [False] * 256  # ì–‘ë°©í–¥ attention

    # â”€â”€â”€ 4-2: Text+State tokens ì¶”ê°€ â”€â”€â”€
    if obs.tokenized_prompt is not None:
        text_tokens = tokenized_inputs  # [4, 200, 2048]  â† Ï€â‚€ëŠ” [4, 16, 2048]
        tokens.append(text_tokens)
        input_mask.append(obs.tokenized_prompt_mask)  # [4, 200]
        ar_mask += [False] * 200  # ì–‘ë°©í–¥ attention

    # â”€â”€â”€ 4-3: Concatenation â”€â”€â”€
    prefix_tokens = jnp.concatenate(tokens, axis=1)
    # [4, 768, 2048] + [4, 200, 2048] = [4, 968, 2048]
    #  ^^^^^^^^^^^^     ^^^^^^^^^^^^
    #  Image (3Ã—256)    Text + State (200)

    prefix_mask = jnp.concatenate(input_mask, axis=1)  # [4, 968]
    prefix_ar_mask = jnp.array(ar_mask)                # [968] (ì „ë¶€ False)

    return prefix_tokens, prefix_mask, prefix_ar_mask

# âœ… Output:
# - prefix_tokens: [4, 968, 2048]
#   â”œâ”€ Image 0 (base):          tokens[0:256]     (256ê°œ)
#   â”œâ”€ Image 1 (left_wrist):    tokens[256:512]   (256ê°œ)
#   â”œâ”€ Image 2 (right_wrist):   tokens[512:768]   (256ê°œ)
#   â””â”€ Text + State:            tokens[768:968]   (200ê°œ)
# - prefix_mask: [4, 968]
# - prefix_ar_mask: [968] (all False = bidirectional)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 4 ìš”ì•½ (Ï€â‚€ vs Ï€â‚€.â‚…):
  Ï€â‚€:   prefix = [image(768), text(16)]   = 784 tokens
  Ï€â‚€.â‚…: prefix = [image(768), text+state(200)] = 968 tokens

  Stateê°€ prefixì— í•©ë¥˜í•¨ìœ¼ë¡œì¨:
  - SuffixëŠ” ì´ì œ ì˜¤ì§ action í† í°ë§Œìœ¼ë¡œ êµ¬ì„±ë¨
  - Prefixâ†”Suffix attentionì„ í†µí•´ actionì´ state ì •ë³´ë¥¼ ì°¸ì¡° ê°€ëŠ¥
  - Prefix ë‚´ë¶€ëŠ” ëª¨ë‘ ì–‘ë°©í–¥ attention (ar_mask ì „ë¶€ False)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ Step 5: Action Embedding + AdaRMSNorm ì¡°ê±´ ìƒì„± (Suffix)

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:139-186`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 5: Actions â†’ Suffix Tokens + time_emb (Ï€â‚€.â‚… ë°©ì‹)
# Ï€â‚€.â‚…ì˜ í•µì‹¬: State í† í° ì—†ìŒ, Timestepì€ AdaRMSNormìœ¼ë¡œ ì£¼ì…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Training input:
actions = raw_input["actions"]  # [4, 50, 32]
state   = observation.state     # [4, 32]  (suffixì— ì‚¬ìš© ì•ˆ í•¨)

# â”€â”€â”€ 5-1: Flow Matching Preparation (Ï€â‚€ì™€ ë™ì¼) â”€â”€â”€
noise = jax.random.normal(noise_rng, actions.shape)  # [4, 50, 32]
time = jax.random.beta(time_rng, 1.5, 1.0, batch_shape=[4]) * 0.999 + 0.001
# time: [4]  ì˜ˆ: [0.234, 0.891, 0.456, 0.123]

time_expanded = time[:, None, None]  # [4, 1, 1]
x_t = time_expanded * noise + (1 - time_expanded) * actions  # [4, 50, 32]
u_t = noise - actions  # [4, 50, 32]  (target velocity)

# â”€â”€â”€ 5-2: Action Token Projection â”€â”€â”€
action_tokens = self.action_in_proj(x_t)
# Linear(32 â†’ 1024)
# [4, 50, 32] â†’ [4, 50, 1024]

# â”€â”€â”€ 5-3: Timestep Embedding (Sinusoidal) â”€â”€â”€
def posemb_sincos(pos, embedding_dim=1024, min_period=4e-3, max_period=4.0):
    fraction = jnp.linspace(0.0, 1.0, embedding_dim // 2)  # [512]
    period = min_period * (max_period / min_period) ** fraction

    sinusoid_input = jnp.einsum("i,j->ij", pos, 1.0 / period * 2 * jnp.pi)  # [4, 512]
    return jnp.concatenate([jnp.sin(sinusoid_input), jnp.cos(sinusoid_input)], axis=-1)
    # [4, 1024]

time_emb = posemb_sincos(time, 1024, min_period=4e-3, max_period=4.0)
# time_emb: [4, 1024]  â† ìŠ¤ì¹¼ë¼ t â†’ 1024ì°¨ì›

# â”€â”€â”€ 5-4: Time MLP (Ï€â‚€.â‚… ì „ìš©) â”€â”€ AdaRMSNorm ì…ë ¥ ìƒì„± â”€â”€â”€
# Ï€â‚€:   time_embë¥¼ actionê³¼ concat í›„ MLPë¡œ actionì— ì§ì ‘ í˜¼í•©
# Ï€â‚€.â‚…: time_embë¥¼ ë³„ë„ MLPë¡œ ì²˜ë¦¬ â†’ adarms_cond ë¡œ ê° layerì— ì£¼ì…
time_emb = self.time_mlp_in(time_emb)    # Linear(1024 â†’ 1024): [4, 1024]
time_emb = nnx.swish(time_emb)
time_emb = self.time_mlp_out(time_emb)   # Linear(1024 â†’ 1024): [4, 1024]
time_emb = nnx.swish(time_emb)
adarms_cond = time_emb                   # [4, 1024] â† ê° Transformer layerì˜ AdaRMS ì¡°ê±´

# â”€â”€â”€ 5-5: Suffix êµ¬ì„± (Ï€â‚€.â‚…: state í† í° ì—†ìŒ) â”€â”€â”€
# Ï€â‚€:   suffix = [state_token(1), action_tokens(32)] = 33 tokens
# Ï€â‚€.â‚…: suffix = [action_tokens(50)]                 = 50 tokens
suffix_tokens = action_tokens  # [4, 50, 1024]
suffix_mask   = jnp.ones([4, 50], dtype=bool)

# AR Mask:
# - ì²« ë²ˆì§¸ action í† í°: True  (prefixê°€ ì´ í† í°ì„ ë³¼ ìˆ˜ ì—†ìŒ)
# - ë‚˜ë¨¸ì§€ action í† í°: False  (actionë¼ë¦¬ ì–‘ë°©í–¥ attention)
suffix_ar_mask = jnp.array([True] + [False] * 49)
# [True, False, False, ..., False]
#  act0  act1   act2        act49

# âœ… Output:
# - suffix_tokens: [4, 50, 1024]    â† Ï€â‚€ëŠ” [4, 33, 1024]
# - suffix_mask:   [4, 50]
# - suffix_ar_mask:[50] ([True, FalseÃ—49])
# - adarms_cond:   [4, 1024]        â† Ï€â‚€ëŠ” None

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 5 ìš”ì•½ (Ï€â‚€ vs Ï€â‚€.â‚…):

  Ï€â‚€ Suffix êµ¬ì„±:
    state_proj:            Linear(7â†’1024) â†’ 1ê°œ state í† í°
    action + time concat:  [action(1024), time(1024)] â†’ MLP â†’ action í† í°ì— time í˜¼í•©
    suffix:                [state(1), action(32)] = 33 tokens
    adarms_cond:           None

  Ï€â‚€.â‚… Suffix êµ¬ì„±:
    action_in_proj:        Linear(32â†’1024) â†’ 50ê°œ action í† í° (time í˜¼í•© ì—†ìŒ)
    time_mlp:              sincos(t) â†’ Linear â†’ SiLU â†’ Linear â†’ SiLU
                           â†’ adarms_cond [4, 1024]  (ê° layerì— ì¡°ê±´ìœ¼ë¡œ ì „ë‹¬)
    suffix:                [action(50)] = 50 tokens (state ì—†ìŒ)

  í•µì‹¬ ì„¤ê³„ ì² í•™ì˜ ì°¨ì´:
    Ï€â‚€:   "timeì„ actionì— ì§ì ‘ ì„ëŠ”ë‹¤"  (suffix levelì—ì„œ ì²˜ë¦¬)
    Ï€â‚€.â‚…: "timeì„ ê° layerì˜ normalizationì— ì¡°ê±´ìœ¼ë¡œ ê±´ë‹¤"  (layer levelì—ì„œ ì²˜ë¦¬)
          â†’ AdaRMSNormì´ ë§¤ layerë§ˆë‹¤ tì— ë”°ë¼ scale/shift/gateë¥¼ ë™ì  ì¡°ì •
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

> **ğŸ”„ í•™ìŠµ vs ì¶”ë¡ **
> | | í•™ìŠµ (Training) | ì¶”ë¡  (Inference) |
> |---|---|---|
> | **ì…ë ¥ actions** | Ground truth actions | ì—†ìŒ (noiseì—ì„œ ì‹œì‘) |
> | **Noise** | `noise ~ N(0, I)` | `noise ~ N(0, I)` (= ì´ˆê¸° x_t) |
> | **Timestep t** | `t ~ Beta(1.5, 1.0)*0.999+0.001` | `t = 1.0, 0.9, ..., 0.0` |
> | **adarms_cond** | time_emb [4, 1024] | ë§¤ step ì¬ê³„ì‚° (t ë°”ë€œ) |

---

## ğŸ“ Step 6: Attention Mask ìƒì„±

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:19-44` + `202-208`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 6: Create Attention Mask (Ï€â‚€.â‚…)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€ 6-1: Concatenate masks â”€â”€â”€
input_mask = jnp.concatenate([prefix_mask, suffix_mask], axis=1)
# [4, 968] + [4, 50] = [4, 1018]
#  ^^^^^^^^   ^^^^^^
#  Prefix      Suffix(action only)

ar_mask = jnp.concatenate([prefix_ar_mask, suffix_ar_mask], axis=0)
# [968] + [50] = [1018]
# ar_mask = [FalseÃ—968, True, FalseÃ—49]
#            ^^^^^^^^    ^^^^  ^^^^^^^^
#            Prefix      act0  act1~49

# â”€â”€â”€ 6-2: cumsumìœ¼ë¡œ ê·¸ë£¹ ë¶„ë¦¬ â”€â”€â”€
cumsum = jnp.cumsum(ar_mask)
# [0Ã—968, 1, 1Ã—49]
# â†’ ê·¸ë£¹ 0: prefix (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸/state)
# â†’ ê·¸ë£¹ 1: action (ëª¨ë‘ ë™ì¼, ì–‘ë°©í–¥)

# Ï€â‚€:   3ê°œ ê·¸ë£¹ (prefix=0, state=1, action=2)
# Ï€â‚€.â‚…: 2ê°œ ê·¸ë£¹ (prefix=0, action=1) â† stateê°€ prefixì— í¬í•¨ë˜ì–´ ì‚¬ë¼ì§

# â”€â”€â”€ 6-3: Attention íŒ¨í„´ ì‹œê°í™” â”€â”€â”€
"""
Attention pattern [1018, 1018]:

                  Prefix(968)           Actions(50)
              cumsum=0                  cumsum=1
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Prefix   â”‚  âœ“  âœ“  âœ“  ...  âœ“        â”‚  âœ—  ...  âœ—  â”‚ cumsum=0
(0-967)  â”‚  (ì–‘ë°©í–¥, img+text+state) â”‚              â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Actions  â”‚  âœ“  âœ“  âœ“  ...  âœ“        â”‚  âœ“  ...  âœ“  â”‚ cumsum=1
(968-1017)â”‚  prefix ì „ì²´ ì°¸ì¡° ê°€ëŠ¥   â”‚  (ì–‘ë°©í–¥)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ê·œì¹™: cumsum[key] <= cumsum[query] â†’ ì°¸ì¡° ê°€ëŠ¥
  prefixâ†’prefix:   0<=0 âœ“ ì–‘ë°©í–¥
  prefixâ†’action:   1<=0 âœ— ì°¨ë‹¨  (prefixëŠ” suffixì— ì˜í–¥ë°›ì§€ ì•ŠìŒ)
  actionâ†’prefix:   0<=1 âœ“ (ì´ë¯¸ì§€, ì–¸ì–´, state ëª¨ë‘ ì°¸ì¡° ê°€ëŠ¥)
  actionâ†’action:   1<=1 âœ“ ì–‘ë°©í–¥
"""

attn_mask = make_attn_mask(input_mask, ar_mask)
# attn_mask: [4, 1018, 1018]
positions = jnp.cumsum(input_mask, axis=1) - 1  # [4, 1018]

# âœ… Output:
# - attn_mask: [4, 1018, 1018] bool
# - positions: [4, 1018] int32

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 6 ìš”ì•½ (Ï€â‚€ vs Ï€â‚€.â‚…):
  Ï€â‚€:   [817, 817] â€” prefix(0) / state(1) / action(2) 3ê·¸ë£¹
  Ï€â‚€.â‚…: [1018, 1018] â€” prefix(0) / action(1) 2ê·¸ë£¹

  Ï€â‚€.â‚…ê°€ ë” ë‹¨ìˆœí•œ ì´ìœ :
  - stateê°€ prefixì— ìˆìœ¼ë¯€ë¡œ ë³„ë„ ê·¸ë£¹ ë¶ˆí•„ìš”
  - actionì€ prefix ì „ì²´(ì´ë¯¸ì§€+ì–¸ì–´+state)ë¥¼ í•œêº¼ë²ˆì— ì°¸ì¡°
  - prefixëŠ” suffixë¥¼ ë³¼ ìˆ˜ ì—†ìŒ (KV Cache ì¬ì‚¬ìš© ê°€ëŠ¥)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ Step 7: Multi-Expert Transformer Layer 0 (AdaRMSNorm)

ì´ì œ 18ê°œì˜ Transformer layer ì¤‘ **ì²« ë²ˆì§¸ layer**ë¥¼ ìì„¸íˆ ë´…ë‹ˆë‹¤.
Ï€â‚€.â‚…ì˜ í•µì‹¬ ë³€í™”: Expert 1(Action Expert)ì— **AdaRMSNorm** ì ìš©.

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:112-131`, `284-333`

### Step 7-1: Pre-Attention AdaRMSNorm

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-1: Pre-Attention Normalization
# Ï€â‚€:   Expert 0, 1 ëª¨ë‘ ì¼ë°˜ RMSNorm (adarms_cond=None)
# Ï€â‚€.â‚…: Expert 0 ì¼ë°˜ RMSNorm, Expert 1 AdaRMSNorm (adarms_cond=time_emb)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì…ë ¥:
xs = [prefix_tokens, suffix_tokens]
# xs[0]: [4, 968, 2048]  (Expert 0 - PaliGemma, width=2048)
# xs[1]: [4, 50, 1024]   (Expert 1 - Action Expert, width=1024)

adarms_cond = [None, time_emb]
# [None, [4, 1024]]
# Expert 0: ì¼ë°˜ RMSNorm (cond=None)
# Expert 1: AdaRMSNorm  (cond=time_emb)

pre_attn = []
gates = []

# â”€â”€â”€ Expert 0: ì¼ë°˜ RMSNorm â”€â”€â”€
var_0 = jnp.mean(jnp.square(xs[0].astype(float32)), axis=-1, keepdims=True)  # [4, 968, 1]
normed_0 = xs[0] * jnp.reciprocal(jnp.sqrt(var_0 + 1e-6))
scale_0 = param("scale", zeros_init(), (2048,))
x_norm_0 = normed_0 * (1 + scale_0)   # [4, 968, 2048]
gate_0 = None                          # Ï€â‚€.â‚… Expert 0ëŠ” gate ì—†ìŒ

# â”€â”€â”€ Expert 1: AdaRMSNorm (Ï€â‚€.â‚… í•µì‹¬) â”€â”€â”€
var_1 = jnp.mean(jnp.square(xs[1].astype(float32)), axis=-1, keepdims=True)  # [4, 50, 1]
normed_1 = xs[1] * jnp.reciprocal(jnp.sqrt(var_1 + 1e-6))

# AdaRMSNorm: time_emb â†’ (scale, shift, gate)ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±
modulation = nn.Dense(1024 * 3, kernel_init=zeros_init)(time_emb)
# Dense(1024 â†’ 3072): [4, 1024] â†’ [4, 3072]
scale_1, shift_1, gate_1 = jnp.split(modulation[:, None, :], 3, axis=-1)
# scale_1, shift_1, gate_1: ê° [4, 1, 1024]

x_norm_1 = normed_1 * (1 + scale_1) + shift_1
# [4, 50, 1024] * [4, 1, 1024] + [4, 1, 1024] = [4, 50, 1024]
# scale_1, shift_1 ì€ ëª¨ë“  50ê°œ action í† í°ì— ë™ì¼í•˜ê²Œ ì ìš©

# âœ… Output:
# x_norm_0: [4, 968, 2048],  gate_0: None
# x_norm_1: [4, 50, 1024],   gate_1: [4, 1, 1024]  â† gateëŠ” residualì—ì„œ ì‚¬ìš©

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 7-1 ìš”ì•½ (Ï€â‚€ vs Ï€â‚€.â‚…):
  Ï€â‚€:   ë‘ Expert ëª¨ë‘ ì¼ë°˜ RMSNorm
          scale íŒŒë¼ë¯¸í„°ë§Œìœ¼ë¡œ ì •ê·œí™” ì¡°ì •
  Ï€â‚€.â‚…: Expert 0 ì¼ë°˜ RMSNorm, Expert 1 AdaRMSNorm
          AdaRMSNormì´ time_embë¡œë¶€í„° scale, shift, gateë¥¼ ìƒì„±:
          - scale_1: ê° ì°¨ì›ë³„ í¬ê¸° ì¡°ì • (tì— ë”°ë¼ ë‹¤ë¦„)
          - shift_1: ê° ì°¨ì›ë³„ í¸í–¥ ì¶”ê°€ (tì— ë”°ë¼ ë‹¤ë¦„)
          - gate_1:  Residual ì—°ê²°ì˜ ê°€ì¤‘ì¹˜ (ë‹¤ìŒ Stepì—ì„œ ì‚¬ìš©)
          â†’ ë§¤ layerë§ˆë‹¤ tì— ë§ëŠ” feature transformationì´ ê°€ëŠ¥
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Step 7-2: QKV Projection (Multi-Expert)

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-2: QKV Projection (Ï€â‚€.â‚… â€” Ï€â‚€ì™€ êµ¬ì¡°ëŠ” ë™ì¼)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€ Expert 0 (PaliGemma, width=2048) â”€â”€â”€
q_0 = q_einsum_0("BTD,NDH->BTNH", x_norm_0)   # [4, 968, 8, 256]
k_0, v_0 = kv_einsum_0("BSD,2KDH->2BSKH", x_norm_0)  # [4, 968, 1, 256]

# â”€â”€â”€ Expert 1 (Action Expert, width=1024) â”€â”€â”€
q_1 = q_einsum_1("BTD,NDH->BTNH", x_norm_1)   # [4, 50, 8, 256]
k_1, v_1 = kv_einsum_1("BSD,2KDH->2BSKH", x_norm_1)  # [4, 50, 1, 256]

# âœ… Q, K, V Output:
# Expert 0: q[4,968,8,256], k[4,968,1,256], v[4,968,1,256]  â† 2048â†’256
# Expert 1: q[4,50,8,256],  k[4,50,1,256],  v[4,50,1,256]   â† 1024â†’256

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 7-2 ìš”ì•½:
  Ï€â‚€.â‚…ë„ Ï€â‚€ì™€ ë™ì¼í•œ êµ¬ì¡° (ì…ë ¥ ì°¨ì›ë§Œ ë‹¤ë¦„):
  - Expert 0: 2048 â†’ 256 (prefix í† í°: 968ê°œ)
  - Expert 1: 1024 â†’ 256 (suffix í† í°: 50ê°œ, Ï€â‚€ëŠ” 33ê°œ)
  - head_dim=256, num_heads=8, num_kv_heads=1 (GQA)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Step 7-3: RoPE

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-3: Concat + RoPE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

q, k, v = (jnp.concatenate(y, axis=1) for y in zip(*qkvs))
# q: [4, 1018, 8, 256]  (968 + 50)  â† Ï€â‚€ëŠ” [4, 817, 8, 256]
# k: [4, 1018, 1, 256]
# v: [4, 1018, 1, 256]

q = _apply_rope(q, positions=positions)  # [4, 1018, 8, 256]
k = _apply_rope(k, positions=positions)  # [4, 1018, 1, 256]
q *= 256 ** -0.5  # 1/âˆšhead_dim ìŠ¤ì¼€ì¼ë§
```

### Step 7-4: Grouped Query Attention

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-4: GQA â€” 1018ê°œ ì „ì²´ í† í°ì— ëŒ€í•´ ê³„ì‚°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

q = einops.rearrange(q, "B T (K G) H -> B T K G H", K=1)
# [4, 1018, 8, 256] â†’ [4, 1018, 1, 8, 256]

logits = jnp.einsum("BTKGH,BSKH->BKGTS", q, k)
# logits: [4, 1, 8, 1018, 1018]

masked_logits = jnp.where(attn_mask[:, None, None, :, :], logits, -2.38e38)
probs = jax.nn.softmax(masked_logits, axis=-1)  # [4, 1, 8, 1018, 1018]

encoded = jnp.einsum("BKGTS,BSKH->BTKGH", probs, v)
encoded = einops.rearrange(encoded, "B T K G H -> B T (K G) H")
# encoded: [4, 1018, 8, 256]

# âœ… í•µì‹¬: action í† í°ì´ prefixì˜ ëª¨ë“  ì •ë³´ (ì´ë¯¸ì§€ + ì–¸ì–´ + state)ë¥¼ ì°¸ì¡°
```

### Step 7-5: Output Projection

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-5: Output Projection (Expertë³„ ë…ë¦½)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Expert 0 (Prefix):
expert_encoded_0 = encoded[:, :968]   # [4, 968, 8, 256]
expert_out_0 = out_einsum_0("BTNH,NHD->BTD", expert_encoded_0)
# [4, 968, 2048]

# Expert 1 (Suffix):
expert_encoded_1 = encoded[:, 968:]   # [4, 50, 8, 256]
expert_out_1 = out_einsum_1("BTNH,NHD->BTD", expert_encoded_1)
# [4, 50, 1024]

# âœ… Output:
# out[0]: [4, 968, 2048]
# out[1]: [4, 50, 1024]
```

### Step 7-6: Gated Residual Connection (Ï€â‚€.â‚… í•µì‹¬)

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-6: Gated Residual Connection
# Ï€â‚€:   ì¼ë°˜ ë§ì…ˆ  (gate=None)
# Ï€â‚€.â‚…: Expert 1ì— gate ì ìš©  (gate=gate_1 from AdaRMSNorm)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _gated_residual(x, y, gate):
    if gate is None:
        return x + y          # Ï€â‚€, Ï€â‚€.â‚… Expert 0
    return x + y * gate       # Ï€â‚€.â‚… Expert 1 ì „ìš©

# Expert 0 (ì¼ë°˜ residual):
xs[0] = xs[0] + expert_out_0
# [4, 968, 2048] + [4, 968, 2048] = [4, 968, 2048]

# Expert 1 (gated residual):
xs[1] = xs[1] + expert_out_1 * gate_1
# [4, 50, 1024] + [4, 50, 1024] * [4, 1, 1024] = [4, 50, 1024]
# gate_1: [4, 1, 1024] â†’ ëª¨ë“  50ê°œ action í† í°ì— ë™ì¼í•œ gate ì ìš©

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 7-6 ìš”ì•½ (Ï€â‚€ vs Ï€â‚€.â‚…):
  Ï€â‚€:   xs[i] = xs[i] + attn_out[i]                 (ë‹¨ìˆœ í•©)
  Ï€â‚€.â‚…: xs[1] = xs[1] + attn_out[1] * gate_1        (gate ê°€ì¤‘ í•©)
        gate_1ì€ AdaRMSNormì´ time_embì—ì„œ ìƒì„±í•œ ë²¡í„°
        â†’ "í˜„ì¬ timestep tì—ì„œ attention ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë°˜ì˜í• ì§€" ë™ì  ì¡°ì •
        â†’ t=1 (pure noise): gateê°€ í° ê°’ â†’ attention ê²°ê³¼ ë§ì´ ë°˜ì˜
        â†’ t=0 (clean data): gateê°€ ì‘ì€ ê°’ â†’ ê¸°ì¡´ í‘œí˜„ ìœ ì§€
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Step 7-7: FeedForward Network (AdaRMSNorm gate ì ìš©)

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 7-7: FFN with AdaRMSNorm gate (Ï€â‚€.â‚…)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

for i, (x, config) in enumerate(zip(xs, configs)):
    # â”€â”€â”€ Pre-FFN AdaRMSNorm â”€â”€â”€
    x_norm, gate = RMSNorm(name=f"pre_ffw_norm_{i}")(x, adarms_cond[i])
    # Expert 0: gate=None  (ì¼ë°˜ RMSNorm)
    # Expert 1: gate=[4, 1, 1024]  (AdaRMSNorm, ìƒˆë¡œìš´ gate ìƒì„±)

    # â”€â”€â”€ GeGLU FeedForward â”€â”€â”€
    if i == 0:  # Expert 0 (PaliGemma, width=2048, mlp_dim=16384)
        ff_gate = jnp.dot(x_norm, w_gating[0])    # [4, 968, 16384]
        ff1     = jnp.dot(x_norm, w_gating[1])    # [4, 968, 16384]
        activations = nn.gelu(ff_gate) * ff1        # [4, 968, 16384]
        outputs = jnp.dot(activations, w_linear)    # [4, 968, 2048]

    else:       # Expert 1 (Action Expert, width=1024, mlp_dim=4096)
        ff_gate = jnp.dot(x_norm, w_gating[0])    # [4, 50, 4096]
        ff1     = jnp.dot(x_norm, w_gating[1])    # [4, 50, 4096]
        activations = nn.gelu(ff_gate) * ff1        # [4, 50, 4096]
        outputs = jnp.dot(activations, w_linear)    # [4, 50, 1024]

# â”€â”€â”€ Second Gated Residual â”€â”€â”€
xs[0] = xs[0] + outputs[0]                          # ì¼ë°˜ residual
xs[1] = xs[1] + outputs[1] * gate_ffn               # gated residual

# âœ… Output (Layer 0 ì™„ë£Œ):
# xs[0]: [4, 968, 2048]
# xs[1]: [4, 50, 1024]

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 7-7 ìš”ì•½:
  Ï€â‚€.â‚… Expert 1ì€ Pre-Attentionê³¼ Pre-FFN ë‘ ë²ˆ AdaRMSNormì„ ì ìš©.
  ê°ê° ë‹¤ë¥¸ (scale, shift, gate) ì„¸íŠ¸ë¥¼ time_embì—ì„œ ìƒì„±.
  â†’ í•œ layerì—ì„œ AdaRMSNormì´ ì´ 2íšŒ ì ìš© (attention ì „, FFN ì „)
  â†’ 18 layers Ã— 2 = ì´ 36íšŒ timestep ì¡°ê±´ ì£¼ì…
  â†’ Ï€â‚€ (action+time MLP 1íšŒ) ë³´ë‹¤ í›¨ì”¬ ì„¸ë°€í•œ timestep ì˜ì¡´ì  ì²˜ë¦¬
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ Step 8: Transformer Layers 1-17

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:365-381` (nn.scan)

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 8: Repeat Layer 0 for Layers 1-17
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# nn.scanìœ¼ë¡œ 18ê°œ layer ë°˜ë³µ (ê° layer ë™ì¼í•œ êµ¬ì¡°, ë‹¤ë¥¸ weight)
for layer_idx in range(1, 18):
    # ë§¤ layerë§ˆë‹¤ ë™ì¼í•˜ê²Œ:
    # 1. Pre-Attention Norm
    #    - Expert 0: ì¼ë°˜ RMSNorm
    #    - Expert 1: AdaRMSNorm(time_emb) â†’ (scale, shift, gate_attn)
    # 2. QKV (Expert 0: 2048â†’256, Expert 1: 1024â†’256)
    # 3. RoPE + concat â†’ [4, 1018, 8, 256]
    # 4. GQA [4, 1, 8, 1018, 1018]
    # 5. Out Projection (Expert 0: 256â†’2048, Expert 1: 256â†’1024)
    # 6. Gated Residual
    #    - Expert 0: ë‹¨ìˆœ í•©
    #    - Expert 1: x + attn_out * gate_attn
    # 7. Pre-FFN Norm
    #    - Expert 0: ì¼ë°˜ RMSNorm
    #    - Expert 1: AdaRMSNorm(time_emb) â†’ (scale, shift, gate_ffn)
    # 8. FFN (Expert 0: 2048â†’16384â†’2048, Expert 1: 1024â†’4096â†’1024)
    # 9. Gated Residual
    #    - Expert 0: ë‹¨ìˆœ í•©
    #    - Expert 1: x + ffn_out * gate_ffn
    pass

# âœ… After 18 layers:
# xs[0]: [4, 968, 2048]
# xs[1]: [4, 50, 1024]

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 8 ìš”ì•½:
  Ï€â‚€.â‚…ëŠ” 18 layers Ã— (attention AdaRMS + FFN AdaRMS) = 36íšŒ time ì¡°ê±´ ì£¼ì….
  ë§¤ layerë¥¼ ê±°ì¹ ìˆ˜ë¡:
  - action í† í°ì´ image/ì–¸ì–´/state prefix ì •ë³´ë¥¼ ê¹Šì´ í†µí•©
  - AdaRMSNorm gateê°€ í˜„ì¬ tì— ë§ê²Œ feature ë³€í™˜ ê°•ë„ë¥¼ ì¡°ì ˆ
  - ê²°ê³¼ì ìœ¼ë¡œ tì— ë”°ë¥¸ ì„¸ë°€í•œ velocity ì˜ˆì¸¡ì´ ê°€ëŠ¥
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ Step 9: Final Layer Normalization

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/gemma.py:409-411`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 9: Final Normalization (Ï€â‚€.â‚…)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

outputs = []
for i, (x, final_norm) in enumerate(zip(xs, self.final_norms)):
    if x is not None:
        adarms_cond_i = adarms_cond[i]  # Expert 0: None, Expert 1: time_emb
        x_final, _ = final_norm(x, adarms_cond_i)
        # Expert 0: ì¼ë°˜ RMSNorm â†’ x_final [4, 968, 2048]
        # Expert 1: AdaRMSNorm  â†’ x_final [4, 50, 1024]
        outputs.append(x_final)

# âœ… Output:
# outputs[0]: [4, 968, 2048]
# outputs[1]: [4, 50, 1024]

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 9 ìš”ì•½:
  Ï€â‚€.â‚…ë„ Final Normì—ì„œ AdaRMSNorm ì ìš© (ì´ 37íšŒ time ì¡°ê±´ ì£¼ì…).
  ì´í›„ action ì˜ˆì¸¡ì—ëŠ” outputs[1] (Expert 1 Suffix) ë§Œ ì‚¬ìš©.
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ Step 10: Velocity Prediction

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:212`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 10: Action Tokens â†’ Velocity Prediction (Ï€â‚€.â‚…)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# suffix_out: [4, 50, 1024]
# Ï€â‚€.â‚…ëŠ” state í† í°ì´ ì—†ìœ¼ë¯€ë¡œ ì „ì²´ 50ê°œë¥¼ ì‚¬ìš©
action_output = suffix_out[:, -50:]  # [4, 50, 1024]
# (Ï€â‚€ëŠ” suffix_out[:, -32:] ìœ¼ë¡œ state í† í° ì œì™¸)

v_t = self.action_out_proj(action_output)
# Linear(1024 â†’ 32)  â† Ï€â‚€ëŠ” Linear(1024 â†’ 7)
# v_t: [4, 50, 32]

# âœ… Output:
# v_t: [4, 50, 32]  (Predicted velocity field)
#      ^  ^^  ^^
#      B  horizon  action_dim

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 10 ìš”ì•½ (Ï€â‚€ vs Ï€â‚€.â‚…):
  Ï€â‚€:   suffix_out[:, -32:] â†’ Linear(1024â†’7)  â†’ v_t [4, 32, 7]
  Ï€â‚€.â‚…: suffix_out[:, -50:] â†’ Linear(1024â†’32) â†’ v_t [4, 50, 32]
  â†’ action_horizon 32â†’50, action_dim 7â†’32 ìœ¼ë¡œ í™•ì¥
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ Step 11: Flow Matching Loss

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:214`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 11: Compute Flow Matching Loss (Ï€â‚€.â‚… â€” Ï€â‚€ì™€ ë™ì¼í•œ ë°©ì‹)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

u_t = noise - actions  # [4, 50, 32]  (target velocity, ì§ì„  ê²½ë¡œ ì ‘ì„ )

loss = jnp.mean(jnp.square(v_t - u_t), axis=-1)
# MSE: [4, 50]  (batch Ã— horizon ë³„ loss)

# âœ… Output:
# loss: [4, 50]

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Step 11 ìš”ì•½:
  Ï€â‚€ì™€ ì™„ì „íˆ ë™ì¼í•œ Flow Matching ëª©í‘œ:
  loss = || v_t - (noise - actions) ||Â²
  ì°¨ì´ëŠ” shapeë¿:
    Ï€â‚€:   [4, 32, 7]
    Ï€â‚€.â‚…: [4, 50, 32]
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“ Inference: Flow Matching Sampling

**ì½”ë“œ ìœ„ì¹˜**: `src/openpi/models/pi0.py:217-279`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Inference: Iterative Denoising (Ï€â‚€.â‚…)
# Ï€â‚€ì™€ êµ¬ì¡°ëŠ” ë™ì¼í•˜ë‚˜ suffix/adarms_cond ë°©ì‹ì´ ë‹¤ë¦„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def sample_actions(self, rng, observation, num_steps=10):
    # â”€â”€â”€ Phase 1: Prefix KV Cache (1íšŒë§Œ ì‹¤í–‰) â”€â”€â”€
    prefix_tokens, prefix_mask, prefix_ar_mask = self.embed_prefix(observation)
    # prefix_tokens: [1, 968, 2048]  (ë°°ì¹˜=1)

    prefix_attn_mask = make_attn_mask(prefix_mask, prefix_ar_mask)
    positions = jnp.cumsum(prefix_mask, axis=1) - 1

    _, kv_cache = self.PaliGemma.llm(
        [prefix_tokens, None],
        mask=prefix_attn_mask,
        positions=positions,
    )
    # kv_cache: 18 layers Ã— [1, 968, 1, 256]  â† Ï€â‚€ë³´ë‹¤ prefixê°€ ë” ê¸¸ì–´ cacheë„ í¼

    # â”€â”€â”€ Phase 2: ì´ˆê¸°í™” â”€â”€â”€
    noise = jax.random.normal(rng, (1, 50, 32))  # [1, 50, 32]  â† Ï€â‚€ëŠ” [1, 32, 7]
    x_t = noise
    dt = -1.0 / num_steps  # -0.1

    # â”€â”€â”€ Phase 3: Iterative Denoising (10íšŒ ë°˜ë³µ) â”€â”€â”€
    def step(carry):
        x_t, time = carry

        # Suffix embedding (Ï€â‚€.â‚… ë°©ì‹)
        suffix_tokens, suffix_mask, suffix_ar_mask, adarms_cond = self.embed_suffix(
            observation, x_t, jnp.broadcast_to(time, 1)
        )
        # suffix_tokens: [1, 50, 1024]   â† Ï€â‚€ëŠ” [1, 33, 1024]
        # adarms_cond:   [1, 1024]        â† Ï€â‚€ëŠ” None (timeì€ ì´ë¯¸ suffixì— í˜¼í•©ë¨)

        suffix_attn_mask = make_attn_mask(suffix_mask, suffix_ar_mask)
        prefix_attn_mask = einops.repeat(prefix_mask, "b p -> b s p", s=50)
        full_attn_mask = jnp.concatenate([prefix_attn_mask, suffix_attn_mask], axis=-1)
        # [1, 50, 968+50] = [1, 50, 1018]

        positions = jnp.sum(prefix_mask, axis=-1)[:, None] + jnp.cumsum(suffix_mask, axis=-1) - 1

        # Transformer (Expert 1ë§Œ, KV cache ì¬ì‚¬ìš©!)
        (prefix_out, suffix_out), _ = self.PaliGemma.llm(
            [None, suffix_tokens],
            mask=full_attn_mask,
            positions=positions,
            kv_cache=kv_cache,
            adarms_cond=[None, adarms_cond],  # â† Ï€â‚€.â‚…ë§Œì˜ í•µì‹¬: time ì¡°ê±´ ì „ë‹¬
        )

        v_t = self.action_out_proj(suffix_out[:, -50:])
        # Linear(1024 â†’ 32): [1, 50, 32]

        return x_t + dt * v_t, time + dt

    def cond(carry):
        x_t, time = carry
        return time >= -dt / 2

    x_0, _ = jax.lax.while_loop(cond, step, (noise, 1.0))
    return x_0  # [1, 50, 32]  â† Denoised actions!

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Inference ìš”ì•½ (Ï€â‚€ vs Ï€â‚€.â‚…):
  ê³µí†µ:
    [Phase 1] prefix KV Cache 1íšŒ ìƒì„±
    [Phase 2] 10íšŒ Euler loop: suffix ì¬ìƒì„± â†’ v_t ì˜ˆì¸¡ â†’ Euler step

  Ï€â‚€.â‚… ì°¨ì´ì :
    - prefix cacheê°€ ë” í¼: 18 Ã— [1, 968, 1, 256] (Ï€â‚€ëŠ” 18 Ã— [1, 784, 1, 256])
    - suffixê°€ ë” ì§§ì•„ì§:   [1, 50, 1024] (Ï€â‚€ëŠ” [1, 33, 1024])
    - adarms_condê°€ ë§¤ step ì¬ê³„ì‚°: sincos(t) â†’ time_mlp â†’ [1, 1024]
      ê° layerì˜ AdaRMSNormì— ì „ë‹¬ â†’ tì— ë”°ë¥¸ ì„¸ë°€í•œ feature ì¡°ì •
    - ì¶œë ¥: [1, 50, 32] (Ï€â‚€ëŠ” [1, 32, 7])
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“Š ì „ì²´ ë°ì´í„° íë¦„ ìš”ì•½

### Shape ë³€í™” ì¶”ì 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Input Data (Ï€â‚€.â‚…)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Images:            3 Ã— [4, 224, 224, 3]  uint8 [0, 255]        â”‚
â”‚ State (as text):   [4, 32] â†’ ì§ë ¬í™” â†’ tokenized_promptì— í¬í•¨   â”‚
â”‚ tokenized_prompt:  [4, 200]              int32                  â”‚
â”‚ Actions:           [4, 50, 32]           float32 (training)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Step 1: Preprocessing                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Images â†’ float32 [-1, 1]                                        â”‚
â”‚ State ì´ë¯¸ tokenized_promptì— í¬í•¨ë¨                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Step 2-4: Prefix Embedding (Ï€â‚€.â‚…)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Images:  3 Ã— [4, 256, 2048]    (SigLIP â†’ 2048 proj)            â”‚
â”‚ Text+State: [4, 200, 2048]     (Embedder, stateê°€ í…ìŠ¤íŠ¸ë¡œ í¬í•¨) â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚ Prefix:  [4, 968, 2048]        (Concatenated)                   â”‚
â”‚          Expert 0 (PaliGemma 2B) ì²˜ë¦¬                           â”‚
â”‚          Ï€â‚€ëŠ” [4, 784, 2048]                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Step 5: Suffix Embedding (Ï€â‚€.â‚…)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Actions: [4,50,32] â†’ Linear(32â†’1024) â†’ [4, 50, 1024]          â”‚
â”‚ Time:    sincos(t)[1024] â†’ time_mlp â†’ adarms_cond[4, 1024]     â”‚
â”‚          (actionì— ì§ì ‘ í˜¼í•© ì•ˆ í•¨ â†’ AdaRMSNormìœ¼ë¡œ ì „ë‹¬)        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚ Suffix:  [4, 50, 1024]   (action 50ê°œ, state í† í° ì—†ìŒ)         â”‚
â”‚          Expert 1 (Action Expert 300M) ì²˜ë¦¬                     â”‚
â”‚          Ï€â‚€ëŠ” [4, 33, 1024] (state 1 + action 32)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Step 6: Attention Mask Generation (Ï€â‚€.â‚…)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mask:      [4, 1018, 1018]                                      â”‚
â”‚ Positions: [4, 1018]                                            â”‚
â”‚                                                                  â”‚
â”‚ 2ê°œ ê·¸ë£¹: prefix(0) / action(1)                                  â”‚
â”‚          Ï€â‚€ëŠ” 3ê°œ ê·¸ë£¹: prefix(0) / state(1) / action(2)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Step 7-8: Multi-Expert Transformer (18 layers, Ï€â‚€.â‚…)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ xs[0]: [4, 968, 2048] â”€â”€â†’ ... â”€â”€â†’ [4, 968, 2048]               â”‚
â”‚        Expert 0: ì¼ë°˜ RMSNorm, QKV 2048â†’256, FFN 2048â†’16384     â”‚
â”‚                                                                  â”‚
â”‚ xs[1]: [4, 50, 1024]  â”€â”€â†’ ... â”€â”€â†’ [4, 50, 1024]                â”‚
â”‚        Expert 1: AdaRMSNorm(time_emb), QKV 1024â†’256,            â”‚
â”‚                  FFN 1024â†’4096, Gated Residual                   â”‚
â”‚                                                                  â”‚
â”‚ AdaRMSNorm ì¡°ê±´ ì£¼ì… íšŸìˆ˜: 18 layers Ã— 2 + final = 37íšŒ         â”‚
â”‚ Attention: Q,K concat â†’ [4, 1018, 8, 256] (ê³µìœ  ê³„ì‚°)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Step 9: Final Normalization (Ï€â‚€.â‚…)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Prefix:  [4, 968, 2048]  (ì¼ë°˜ RMSNorm)                         â”‚
â”‚ Suffix:  [4, 50, 1024]   (AdaRMSNorm)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Step 10: Velocity Prediction (Ï€â‚€.â‚…)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ suffix_out[:, -50:] â†’ Linear(1024â†’32) â†’ v_t: [4, 50, 32]      â”‚
â”‚ Ï€â‚€ëŠ”: suffix_out[:, -32:] â†’ Linear(1024â†’7) â†’ v_t: [4, 32, 7]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Step 11: Loss Computation (Ï€â‚€.â‚…)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ loss = MSE(v_t, noise - actions)                                â”‚
â”‚      = mean((v_t - (noise - actions))^2, axis=-1)              â”‚
â”‚ loss: [4, 50]                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¥ Ï€â‚€ vs Ï€â‚€.â‚… ì „ì²´ ë¹„êµ

### ì•„í‚¤í…ì²˜ ì°¨ì´

```python
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ï€â‚€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Pi0Config(
    pi05=False,
    paligemma_variant="gemma_2b",      # PaliGemma 2B (width=2048)
    action_expert_variant="gemma_300m", # Action Expert 300M (width=1024)
    action_dim=32,
    action_horizon=50,
    max_token_len=48,                  # ì§§ìŒ (í…ìŠ¤íŠ¸ë§Œ)
    discrete_state_input=False,        # stateëŠ” suffixì— continuous í† í°
)
# state ì²˜ë¦¬: Linear(action_dimâ†’1024) â†’ 1ê°œ suffix í† í°
# time ì²˜ë¦¬:  sincos(t) + action concat â†’ MLP â†’ action í† í°ì— í˜¼í•©

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ï€â‚€.â‚… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Pi0Config(
    pi05=True,
    paligemma_variant="gemma_2b",      # ë™ì¼
    action_expert_variant="gemma_300m", # ë™ì¼
    action_dim=32,
    action_horizon=50,
    max_token_len=200,                 # ê¸¸ì–´ì§ (í…ìŠ¤íŠ¸+state)
    discrete_state_input=True,         # stateëŠ” prefixì— í…ìŠ¤íŠ¸ í† í°
)
# state ì²˜ë¦¬: í…ìŠ¤íŠ¸ ì§ë ¬í™” â†’ tokenized_prompt ì— í¬í•¨ â†’ prefix
# time ì²˜ë¦¬:  sincos(t) â†’ time_mlp â†’ adarms_cond â†’ ê° layer AdaRMSNorm
```

### ì‹œí€€ìŠ¤ ê¸¸ì´ ë¹„êµ

```
Ï€â‚€:
  Prefix:  image(768) + text(16)          = 784 tokens
  Suffix:  state(1)   + action(32)        =  33 tokens
  Total:                                  = 817 tokens
  Attention mask: [817, 817]

Ï€â‚€.â‚…:
  Prefix:  image(768) + text+state(200)   = 968 tokens
  Suffix:  action(50)                     =  50 tokens
  Total:                                  = 1018 tokens
  Attention mask: [1018, 1018]
```

### Timestep ì£¼ì… ë°©ì‹ ë¹„êµ

```
Ï€â‚€  (suffix level ì²˜ë¦¬):
  time_emb [4,1024] â†’ repeat(s=32) â†’ concat([action, time]) [4,32,2048]
                    â†’ action_time_mlp_in (2048â†’1024) + SiLU
                    â†’ action_time_mlp_out (1024â†’1024)
                    â†’ action í† í° ìì²´ì— time ì •ë³´ê°€ í˜¼í•©ë¨
  ì´ time ì¡°ê±´ ì£¼ì…: 1íšŒ (suffix ìƒì„± ì‹œ)

Ï€â‚€.â‚… (layer level ì²˜ë¦¬, AdaRMSNorm):
  time_emb [4,1024] â†’ time_mlp_in (1024â†’1024) + SiLU
                    â†’ time_mlp_out (1024â†’1024) + SiLU
                    â†’ adarms_cond [4,1024]
  ë§¤ Transformer layerì—ì„œ:
    - Pre-Attention AdaRMSNorm: scale, shift, gate_attn
    - Pre-FFN AdaRMSNorm:       scale, shift, gate_ffn
  ì´ time ì¡°ê±´ ì£¼ì…: 18 Ã— 2 + 1(final) = 37íšŒ
```

### AdaRMSNorm ì‘ë™ ì›ë¦¬

```python
# ì¼ë°˜ RMSNorm (Ï€â‚€):
scale = param([1024])          # í•™ìŠµ ê°€ëŠ¥, tì™€ ë¬´ê´€
x_norm = x / rms(x) * (1 + scale)

# AdaRMSNorm (Ï€â‚€.â‚…):
modulation = Dense(1024â†’3072)(time_emb)    # tì— ë”°ë¼ ë™ì  ìƒì„±
scale, shift, gate = split(modulation, 3)  # ê° [4, 1, 1024]

x_norm = x / rms(x) * (1 + scale) + shift  # tì— ë”°ë¥¸ feature ì¡°ì •
residual = prev_x + output * gate           # tì— ë”°ë¥¸ residual ê°•ë„

# ì§ê´€:
# scale: í˜„ì¬ tì—ì„œ ì–´ë–¤ featureë¥¼ ê°•ì¡°í• ì§€
# shift: í˜„ì¬ tì—ì„œ featureì˜ ê¸°ì¤€ì ì„ ì–´ë””ë¡œ ì˜®ê¸¸ì§€
# gate:  í˜„ì¬ tì—ì„œ attention/FFN ê²°ê³¼ë¥¼ ì–¼ë§ˆë‚˜ ë°˜ì˜í• ì§€
```

---

## ğŸ“ ë³€ê²½ ì´ë ¥

- 2026-02-15: ì´ˆì•ˆ ì‘ì„±
  - pi0_description.mdë¥¼ ê¸°ë°˜ìœ¼ë¡œ Ï€â‚€.â‚… ì „ìš© ë¬¸ì„œ ì‘ì„±
  - Ï€â‚€ vs Ï€â‚€.â‚… ì°¨ì´ì  ìƒì„¸ ë¶„ì„
  - AdaRMSNorm ë™ì‘ ì›ë¦¬ ìƒì„¸ ì„¤ëª…
  - State í…ìŠ¤íŠ¸ ì§ë ¬í™” ë°©ì‹ ì„¤ëª…
  - ì „ì²´ shape ë³€í™” ì¶”ì  (Ï€â‚€ì™€ ë¹„êµ í¬í•¨)

---

**ì‘ì„±ì**: AI Analysis
**í”„ë¡œì íŠ¸**: openpi (Physical Intelligence)
**ë²„ì „**: 1.0
